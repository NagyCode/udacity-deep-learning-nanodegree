{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 1:\n",
      "Image - Min Value: 5 Max Value: 254\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fac91f828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 1\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ret=[]\n",
    "    for image in x: \n",
    "        ret.append(numpy.divide(image,255.))\n",
    "    return numpy.asarray(ret)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    arr=numpy.asarray(x)\n",
    "    b = np.zeros((len(arr), 10))\n",
    "    b[np.arange(len(arr)), x] = 1\n",
    "    return b\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    pool_ksize = [1, *pool_ksize, 1]\n",
    "    pool_strides = [1, *pool_strides, 1]\n",
    "\n",
    "    \n",
    "    conv_strides = [1, *conv_strides, 1]\n",
    "    \n",
    "    W = tf.Variable(tf.random_normal([*conv_ksize, int(x_tensor.shape[3]),conv_num_outputs],stddev=0.01))\n",
    "    b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    x = tf.nn.conv2d(x_tensor, W, strides=conv_strides, padding='SAME')    \n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.max_pool(x, pool_ksize, pool_strides, padding='SAME')\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    _, *image_size = x_tensor.get_shape().as_list()\n",
    "    image_size = image_size[0] * image_size[1] * image_size[2]\n",
    "    return tf.reshape(x_tensor, [-1, image_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    W = tf.Variable(tf.random_normal((int(x_tensor.get_shape().as_list()[1]), num_outputs),stddev=0.01))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    x = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    W = tf.Variable(tf.random_normal((int(x_tensor.get_shape().as_list()[1]), num_outputs),stddev=0.01))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    x = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    with tf.device('/gpu:0'):\n",
    "        \"\"\"\n",
    "        Create a convolutional neural network model\n",
    "        : x: Placeholder tensor that holds image data.\n",
    "        : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "        : return: Tensor that represents logits\n",
    "        \"\"\"\n",
    "        # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "        #    Play around with different number of outputs, kernel size and stride\n",
    "        # Function Definition from Above:\n",
    "        #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "        x = conv2d_maxpool(x, 32, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "        x = conv2d_maxpool(x, 32, (3, 3), (2, 2), (2, 2), (2, 2))\n",
    "        x = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "\n",
    "        # TODO: Apply a Flatten Layer\n",
    "        # Function Definition from Above:\n",
    "        #   flatten(x_tensor)\n",
    "        x = flatten(x)\n",
    "\n",
    "        # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "        #    Play around with different number of outputs\n",
    "        # Function Definition from Above:\n",
    "        #   fully_conn(x_tensor, num_outputs)\n",
    "        x = fully_conn(x, 512)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        x = fully_conn(x, 128)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "        # TODO: Apply an Output Layer\n",
    "        #    Set this to the number of classes\n",
    "        # Function Definition from Above:\n",
    "        #   output(x_tensor, num_outputs)\n",
    "        a = output(x, 10)\n",
    "\n",
    "        # TODO: return output\n",
    "        return a\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    l = session.run(cost, feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.})\n",
    "    \n",
    "    acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1. })\n",
    "\n",
    "    print('Loss: {:>10.4f} accuracy: {:.6f}'.format(l, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3028 accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2740 accuracy: 0.117400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1594 accuracy: 0.181400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1353 accuracy: 0.172000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0904 accuracy: 0.217000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.0562 accuracy: 0.230400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0433 accuracy: 0.237200\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0310 accuracy: 0.233400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0186 accuracy: 0.246200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0015 accuracy: 0.242600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.9878 accuracy: 0.253800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.9694 accuracy: 0.261800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.9522 accuracy: 0.281800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.9306 accuracy: 0.286200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.9222 accuracy: 0.290000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.9019 accuracy: 0.295800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.8927 accuracy: 0.303600\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.8688 accuracy: 0.316600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.8565 accuracy: 0.322400\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8303 accuracy: 0.331200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.8117 accuracy: 0.342200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.7837 accuracy: 0.354000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.7669 accuracy: 0.360000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.7577 accuracy: 0.363400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.7296 accuracy: 0.373200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.7241 accuracy: 0.376400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.7026 accuracy: 0.381600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.6914 accuracy: 0.387600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.6781 accuracy: 0.390600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.6641 accuracy: 0.393800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.6513 accuracy: 0.397400\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.6386 accuracy: 0.401200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.6215 accuracy: 0.399800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.6158 accuracy: 0.407200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.5967 accuracy: 0.411800\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.5978 accuracy: 0.416200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.5705 accuracy: 0.417200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.5537 accuracy: 0.416600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.5449 accuracy: 0.416200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.5440 accuracy: 0.423400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.5440 accuracy: 0.411000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.5316 accuracy: 0.416400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.4959 accuracy: 0.435600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.4909 accuracy: 0.435800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.4777 accuracy: 0.440000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.4680 accuracy: 0.443400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.4542 accuracy: 0.446400\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.4475 accuracy: 0.450400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.4323 accuracy: 0.446800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.4168 accuracy: 0.454000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.4031 accuracy: 0.456400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.3889 accuracy: 0.459200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.3839 accuracy: 0.462400\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.3473 accuracy: 0.468400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.3355 accuracy: 0.469200\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.3284 accuracy: 0.468600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.3115 accuracy: 0.470000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.3090 accuracy: 0.466400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2725 accuracy: 0.474600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2815 accuracy: 0.477000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.2469 accuracy: 0.481800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2351 accuracy: 0.478400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.2122 accuracy: 0.484400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.2128 accuracy: 0.486400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.1958 accuracy: 0.494400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.1859 accuracy: 0.486800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.1512 accuracy: 0.494600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.1713 accuracy: 0.489800\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.1243 accuracy: 0.492800\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.1331 accuracy: 0.490400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.1164 accuracy: 0.497200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1098 accuracy: 0.498000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.0931 accuracy: 0.498600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.0705 accuracy: 0.498600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.0430 accuracy: 0.504800\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.0213 accuracy: 0.506200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.0086 accuracy: 0.506000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.9947 accuracy: 0.508600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.9769 accuracy: 0.507600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.9780 accuracy: 0.504200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.9648 accuracy: 0.504800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.9386 accuracy: 0.521200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.9041 accuracy: 0.518400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.8975 accuracy: 0.520400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.8879 accuracy: 0.522000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.8670 accuracy: 0.521600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.8489 accuracy: 0.519800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.8346 accuracy: 0.522200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.8434 accuracy: 0.518400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.8277 accuracy: 0.517200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.8359 accuracy: 0.516200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.8358 accuracy: 0.507800\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.9059 accuracy: 0.494200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.8136 accuracy: 0.514200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.8029 accuracy: 0.510800\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.7652 accuracy: 0.523000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.7445 accuracy: 0.516800\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.7332 accuracy: 0.517800\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.7240 accuracy: 0.520400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.7310 accuracy: 0.517400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3027 accuracy: 0.126400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2784 accuracy: 0.109400\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1733 accuracy: 0.170000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.0474 accuracy: 0.230000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.0250 accuracy: 0.249000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0534 accuracy: 0.251800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9821 accuracy: 0.253000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.9595 accuracy: 0.267200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.9044 accuracy: 0.269200\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.9435 accuracy: 0.271200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9536 accuracy: 0.265600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.8777 accuracy: 0.276000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.8052 accuracy: 0.276000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.7635 accuracy: 0.283000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.8466 accuracy: 0.299200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.8792 accuracy: 0.314600\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.7969 accuracy: 0.321000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.6705 accuracy: 0.326200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.6566 accuracy: 0.352800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.7512 accuracy: 0.340800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.7861 accuracy: 0.353400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.7276 accuracy: 0.361000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.5668 accuracy: 0.361400\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.5646 accuracy: 0.366600\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6553 accuracy: 0.370600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6819 accuracy: 0.387600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.6577 accuracy: 0.390000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.4691 accuracy: 0.381600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.4960 accuracy: 0.386800\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.5840 accuracy: 0.404400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.6202 accuracy: 0.409400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.6088 accuracy: 0.410000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.4370 accuracy: 0.402600\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.4364 accuracy: 0.411200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.5373 accuracy: 0.423400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.5527 accuracy: 0.429600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.5514 accuracy: 0.435200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.3331 accuracy: 0.432400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.3864 accuracy: 0.435400\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.4972 accuracy: 0.434200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.5091 accuracy: 0.450600\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.5253 accuracy: 0.451600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3034 accuracy: 0.457600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.3376 accuracy: 0.454200\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.4589 accuracy: 0.459200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.4889 accuracy: 0.467000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.4681 accuracy: 0.469200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.2566 accuracy: 0.468200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.2979 accuracy: 0.474800\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.4063 accuracy: 0.471400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.4326 accuracy: 0.482200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.4392 accuracy: 0.468800\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.2241 accuracy: 0.480400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.2644 accuracy: 0.487400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.3693 accuracy: 0.487600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.3903 accuracy: 0.495200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.3995 accuracy: 0.476400\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.2069 accuracy: 0.490400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.2262 accuracy: 0.497400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.3344 accuracy: 0.490000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.3454 accuracy: 0.498600\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.3559 accuracy: 0.510400\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.1914 accuracy: 0.496200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.2009 accuracy: 0.493600\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3145 accuracy: 0.503800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.3227 accuracy: 0.504800\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.3264 accuracy: 0.517600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.1675 accuracy: 0.512400\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.1617 accuracy: 0.517000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.2951 accuracy: 0.510800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.3021 accuracy: 0.525000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.3064 accuracy: 0.524600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.1341 accuracy: 0.526800\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.1323 accuracy: 0.529400\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.2609 accuracy: 0.518200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.2789 accuracy: 0.515800\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.2659 accuracy: 0.531600\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1068 accuracy: 0.528000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.0953 accuracy: 0.537800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.2238 accuracy: 0.523600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2604 accuracy: 0.534200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.2484 accuracy: 0.544800\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.0626 accuracy: 0.541800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.0646 accuracy: 0.547200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.1817 accuracy: 0.536800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2175 accuracy: 0.541000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.2115 accuracy: 0.547800\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.0591 accuracy: 0.546000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.0381 accuracy: 0.550800\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.1615 accuracy: 0.543600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1887 accuracy: 0.554000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.2103 accuracy: 0.545800\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.0625 accuracy: 0.539200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0424 accuracy: 0.551800\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.1479 accuracy: 0.549000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1733 accuracy: 0.550400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.1753 accuracy: 0.541600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.0492 accuracy: 0.548400\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0059 accuracy: 0.561400\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.1219 accuracy: 0.556000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.1640 accuracy: 0.559200\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.1349 accuracy: 0.558800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.0435 accuracy: 0.546400\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.0047 accuracy: 0.563000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.1088 accuracy: 0.558400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.1539 accuracy: 0.555200\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.1213 accuracy: 0.574200\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.0236 accuracy: 0.552800\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.9990 accuracy: 0.570000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.0787 accuracy: 0.566400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.1195 accuracy: 0.554200\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.1108 accuracy: 0.567800\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.9616 accuracy: 0.568600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.9689 accuracy: 0.577000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.0705 accuracy: 0.565200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.1045 accuracy: 0.560600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.1444 accuracy: 0.554200\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.9377 accuracy: 0.575600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.9553 accuracy: 0.577200\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.0313 accuracy: 0.577800\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.0919 accuracy: 0.568200\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.1131 accuracy: 0.564200\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.9099 accuracy: 0.580400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.9221 accuracy: 0.583400\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.9980 accuracy: 0.585200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.0858 accuracy: 0.575000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.0690 accuracy: 0.571800\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.9171 accuracy: 0.584800\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.8980 accuracy: 0.597600\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.9814 accuracy: 0.585000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.0554 accuracy: 0.587800\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.0448 accuracy: 0.576400\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.9062 accuracy: 0.582400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.8829 accuracy: 0.598400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.9883 accuracy: 0.585000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.0360 accuracy: 0.591200\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.0160 accuracy: 0.584800\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.8933 accuracy: 0.588000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.8858 accuracy: 0.587800\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.9792 accuracy: 0.588800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.0224 accuracy: 0.590600\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.0249 accuracy: 0.586200\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8917 accuracy: 0.584600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.8518 accuracy: 0.600200\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.9524 accuracy: 0.590800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.0000 accuracy: 0.600200\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.0093 accuracy: 0.587200\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.8643 accuracy: 0.588600\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8411 accuracy: 0.605400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.9291 accuracy: 0.597000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9943 accuracy: 0.605000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.9989 accuracy: 0.589800\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.8521 accuracy: 0.594200\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.8238 accuracy: 0.611400\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.9084 accuracy: 0.601600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.9624 accuracy: 0.612000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.9751 accuracy: 0.598000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.8413 accuracy: 0.597400\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.8194 accuracy: 0.610600\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.8901 accuracy: 0.600200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.9387 accuracy: 0.612200\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.9554 accuracy: 0.597800\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.8278 accuracy: 0.602000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.8156 accuracy: 0.610800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.8702 accuracy: 0.604000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.9191 accuracy: 0.613400\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.9181 accuracy: 0.609600\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.8095 accuracy: 0.607200\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.7995 accuracy: 0.613600\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.8562 accuracy: 0.611200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.9100 accuracy: 0.607600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.9377 accuracy: 0.603000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.7895 accuracy: 0.609400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.8021 accuracy: 0.609200\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.8300 accuracy: 0.615400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.9247 accuracy: 0.612600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.9524 accuracy: 0.596400\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.7576 accuracy: 0.617400\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.7833 accuracy: 0.618400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.8369 accuracy: 0.618800\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8917 accuracy: 0.612400\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.9060 accuracy: 0.608200\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.7595 accuracy: 0.614000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.8046 accuracy: 0.619000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.8241 accuracy: 0.619400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.8651 accuracy: 0.610400\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.8788 accuracy: 0.616200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.7542 accuracy: 0.616000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7552 accuracy: 0.626000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.7829 accuracy: 0.625600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.8428 accuracy: 0.625000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.8738 accuracy: 0.618400\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.7589 accuracy: 0.616800\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.7580 accuracy: 0.622400\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.7824 accuracy: 0.625000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.8399 accuracy: 0.620400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.8697 accuracy: 0.625800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7450 accuracy: 0.622800\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7839 accuracy: 0.613200\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.7697 accuracy: 0.628000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.8348 accuracy: 0.614000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.8826 accuracy: 0.617000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.7204 accuracy: 0.622800\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.7563 accuracy: 0.624000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.7506 accuracy: 0.628200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.8117 accuracy: 0.620400\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.8256 accuracy: 0.626200\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.7165 accuracy: 0.621800\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.7599 accuracy: 0.625000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.7679 accuracy: 0.624800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8083 accuracy: 0.627200\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.8317 accuracy: 0.618200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.7253 accuracy: 0.624200\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.7370 accuracy: 0.633800\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.7592 accuracy: 0.621200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.7860 accuracy: 0.630600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.8150 accuracy: 0.625200\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.6892 accuracy: 0.632600\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.7313 accuracy: 0.632000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.7326 accuracy: 0.624400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.7914 accuracy: 0.634000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.7662 accuracy: 0.632600\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.6949 accuracy: 0.624400\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.7197 accuracy: 0.634200\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.7161 accuracy: 0.629800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.7520 accuracy: 0.642800\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.7669 accuracy: 0.640600\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.7119 accuracy: 0.621600\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.6962 accuracy: 0.637000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.7306 accuracy: 0.626600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.7446 accuracy: 0.643800\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.7887 accuracy: 0.625600\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.6756 accuracy: 0.630600\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.6992 accuracy: 0.630800\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.6959 accuracy: 0.635800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.7405 accuracy: 0.639400\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.7585 accuracy: 0.639000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.6515 accuracy: 0.639000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.6885 accuracy: 0.627400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6683 accuracy: 0.645600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.7421 accuracy: 0.643200\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.7546 accuracy: 0.641400\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.6344 accuracy: 0.639600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.6521 accuracy: 0.643800\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.6681 accuracy: 0.639000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.7335 accuracy: 0.642800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.7238 accuracy: 0.643400\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.6150 accuracy: 0.641400\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.6383 accuracy: 0.646000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.6596 accuracy: 0.634600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.7266 accuracy: 0.642200\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.7178 accuracy: 0.644800\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.6210 accuracy: 0.639000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.6257 accuracy: 0.645000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6875 accuracy: 0.626000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.7134 accuracy: 0.643800\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.7113 accuracy: 0.641600\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.6049 accuracy: 0.638800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.6207 accuracy: 0.645400\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.6593 accuracy: 0.630800\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.7156 accuracy: 0.644400\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.7150 accuracy: 0.647400\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.6022 accuracy: 0.637200\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6453 accuracy: 0.633000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.6356 accuracy: 0.644800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.7020 accuracy: 0.641600\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.7192 accuracy: 0.649200\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.5929 accuracy: 0.641600\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6148 accuracy: 0.640000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.6156 accuracy: 0.657000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.6770 accuracy: 0.652600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.7023 accuracy: 0.649400\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.5756 accuracy: 0.650000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6046 accuracy: 0.643800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.6103 accuracy: 0.652600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.6861 accuracy: 0.650400\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.6761 accuracy: 0.650000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.5694 accuracy: 0.653000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.5766 accuracy: 0.647200\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.5870 accuracy: 0.654400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.6677 accuracy: 0.655000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.6613 accuracy: 0.652400\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.5364 accuracy: 0.655000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.5579 accuracy: 0.652000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.5709 accuracy: 0.658400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6494 accuracy: 0.646000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.6492 accuracy: 0.650200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.5386 accuracy: 0.662800\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.5436 accuracy: 0.651800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.5520 accuracy: 0.663200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.6436 accuracy: 0.656600\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.6335 accuracy: 0.652200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.5206 accuracy: 0.661200\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.5347 accuracy: 0.661000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.5301 accuracy: 0.661000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.6302 accuracy: 0.663200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.6130 accuracy: 0.650400\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.5200 accuracy: 0.659800\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.5409 accuracy: 0.656400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.5420 accuracy: 0.664600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.6080 accuracy: 0.652400\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.5815 accuracy: 0.658000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.4991 accuracy: 0.664600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.5236 accuracy: 0.658200\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.5495 accuracy: 0.658000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.6110 accuracy: 0.655600\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.5772 accuracy: 0.655000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.5002 accuracy: 0.663600\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.5122 accuracy: 0.662000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.5214 accuracy: 0.657400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.5956 accuracy: 0.658000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.5835 accuracy: 0.652600\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.4740 accuracy: 0.664600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.4913 accuracy: 0.663000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.5163 accuracy: 0.656200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.5832 accuracy: 0.657400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.5350 accuracy: 0.656000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.5103 accuracy: 0.661000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.5067 accuracy: 0.654600\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.5221 accuracy: 0.656800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.5971 accuracy: 0.658800\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.5639 accuracy: 0.652200\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.4802 accuracy: 0.661800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.4865 accuracy: 0.667200\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.5057 accuracy: 0.661400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.5722 accuracy: 0.658600\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.5453 accuracy: 0.659400\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.4548 accuracy: 0.672600\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.4916 accuracy: 0.664200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.4842 accuracy: 0.666200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.5590 accuracy: 0.656200\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.5338 accuracy: 0.659800\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.4530 accuracy: 0.667800\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.4685 accuracy: 0.661000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.4704 accuracy: 0.663000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.5785 accuracy: 0.653000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.5188 accuracy: 0.655600\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.4385 accuracy: 0.668400\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.4922 accuracy: 0.655400\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.4582 accuracy: 0.665200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.5614 accuracy: 0.659200\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.5170 accuracy: 0.655600\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.4431 accuracy: 0.668600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.4985 accuracy: 0.659800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.4522 accuracy: 0.666400\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.5725 accuracy: 0.647000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.5218 accuracy: 0.655200\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.4496 accuracy: 0.666200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.5108 accuracy: 0.655400\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.4768 accuracy: 0.659200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5414 accuracy: 0.655000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.5348 accuracy: 0.649600\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.4529 accuracy: 0.666000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.5055 accuracy: 0.655400\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.4606 accuracy: 0.660200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.5463 accuracy: 0.650000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.5240 accuracy: 0.661400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.4619 accuracy: 0.667800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.4945 accuracy: 0.659200\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.4772 accuracy: 0.655000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.5195 accuracy: 0.652200\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.5309 accuracy: 0.661000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.4357 accuracy: 0.664800\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.4741 accuracy: 0.666000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.4369 accuracy: 0.664400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.5036 accuracy: 0.657800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.5226 accuracy: 0.661400\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.4108 accuracy: 0.669800\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.4657 accuracy: 0.659400\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.4580 accuracy: 0.655800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.4912 accuracy: 0.660000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.5028 accuracy: 0.660800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.3946 accuracy: 0.663400\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.4302 accuracy: 0.666000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.4253 accuracy: 0.663400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.4759 accuracy: 0.659200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.5196 accuracy: 0.655200\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.4011 accuracy: 0.665800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.4361 accuracy: 0.667000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.4240 accuracy: 0.654200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.4906 accuracy: 0.658400\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.4895 accuracy: 0.656800\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.3797 accuracy: 0.672000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.4207 accuracy: 0.672600\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.4220 accuracy: 0.657200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.4775 accuracy: 0.665000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.4891 accuracy: 0.653000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.3647 accuracy: 0.667400\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.4152 accuracy: 0.666800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.3948 accuracy: 0.667000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.4756 accuracy: 0.671200\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.4325 accuracy: 0.664400\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.3813 accuracy: 0.663200\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.4540 accuracy: 0.661400\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.4051 accuracy: 0.661400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.4742 accuracy: 0.668600\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.4427 accuracy: 0.667200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.3852 accuracy: 0.660200\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.4399 accuracy: 0.659800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.4141 accuracy: 0.663000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.4787 accuracy: 0.671000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.4277 accuracy: 0.664600\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.3740 accuracy: 0.672600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.4152 accuracy: 0.664400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.3895 accuracy: 0.669400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.4717 accuracy: 0.660400\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.4183 accuracy: 0.667400\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.3481 accuracy: 0.675800\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.4171 accuracy: 0.661800\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.3764 accuracy: 0.665400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.4528 accuracy: 0.664000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.4291 accuracy: 0.663000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.3485 accuracy: 0.676200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.3956 accuracy: 0.667800\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.3947 accuracy: 0.670400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.4410 accuracy: 0.670800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.3893 accuracy: 0.667000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.3449 accuracy: 0.670400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.3658 accuracy: 0.672600\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.3825 accuracy: 0.668000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.4292 accuracy: 0.676200\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.3850 accuracy: 0.674400\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.3430 accuracy: 0.671800\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.4026 accuracy: 0.659800\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.3879 accuracy: 0.662200\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.4440 accuracy: 0.669000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.3977 accuracy: 0.669400\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.3294 accuracy: 0.675600\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.3742 accuracy: 0.665000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.3593 accuracy: 0.666600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.4321 accuracy: 0.669400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.3948 accuracy: 0.671800\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.3130 accuracy: 0.676800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.3812 accuracy: 0.666400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.3490 accuracy: 0.669600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.4227 accuracy: 0.669600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.3737 accuracy: 0.671800\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.3291 accuracy: 0.672600\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.3682 accuracy: 0.665200\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.3738 accuracy: 0.669400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4183 accuracy: 0.669000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.3742 accuracy: 0.670400\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.3227 accuracy: 0.672800\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.3445 accuracy: 0.675400\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.3472 accuracy: 0.667000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.4114 accuracy: 0.667200\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.3652 accuracy: 0.672200\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.3117 accuracy: 0.674800\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.3631 accuracy: 0.664200\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.3546 accuracy: 0.667600\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.4095 accuracy: 0.670000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.3760 accuracy: 0.669000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.3358 accuracy: 0.670800\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.3521 accuracy: 0.668200\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.3573 accuracy: 0.659800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4150 accuracy: 0.657600\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.3654 accuracy: 0.662400\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.3135 accuracy: 0.668800\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.3721 accuracy: 0.663600\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.3331 accuracy: 0.668200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4124 accuracy: 0.658400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.3671 accuracy: 0.665600\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.3262 accuracy: 0.669800\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.3558 accuracy: 0.665200\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.3293 accuracy: 0.673800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4144 accuracy: 0.658600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.3791 accuracy: 0.673000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.3000 accuracy: 0.672400\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.3794 accuracy: 0.662200\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.3238 accuracy: 0.676200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.4058 accuracy: 0.660800\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.3707 accuracy: 0.675200\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.3195 accuracy: 0.670000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.4080 accuracy: 0.658000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.3375 accuracy: 0.678200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3888 accuracy: 0.675800\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.3541 accuracy: 0.671800\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.3158 accuracy: 0.679600\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.3815 accuracy: 0.653400\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.3267 accuracy: 0.679200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3699 accuracy: 0.677600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.3739 accuracy: 0.667600\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.3131 accuracy: 0.671600\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.3485 accuracy: 0.662400\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.3325 accuracy: 0.670200\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3674 accuracy: 0.680000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.3787 accuracy: 0.665000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.3339 accuracy: 0.672400\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.3599 accuracy: 0.651000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.3124 accuracy: 0.673800\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3760 accuracy: 0.676400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.3551 accuracy: 0.670600\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.3129 accuracy: 0.672000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.3299 accuracy: 0.660400\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.3016 accuracy: 0.681400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3636 accuracy: 0.673600\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.3594 accuracy: 0.663200\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.3323 accuracy: 0.665200\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.3350 accuracy: 0.657400\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.3203 accuracy: 0.676600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.668037685751915\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3XecZFWZ//HPU93T0z05ATPEITMSJCOwwiCuCRXMCQUM\nKygGDGteQdewrqusYMJVMWPWnxlBMiKSRHIc4jAwDJNDh3p+fzyn6t6+U1VdPZ27v+/Xq6aq7jn3\n3HOra6qee+oEc3dERERERARKI10BEREREZHRQsGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWERER\nEUkUHIuIiIiIJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKS\nKDgWEREREUkUHIuIiIiIJAqOR5iZ7WRmLzWz08zsQ2b2QTN7h5m9wswONrNpI13HesysZGbHm9kF\nZnaPma02M8/dfjXSdRQZbcxsYeH/yZmDkXe0MrPFhXM4eaTrJCLSSOtIV2AiMrM5wGnAW4Cd+she\nNrPbgCuA3wEXu/vGIa5in9I5/Aw4ZqTrIsPPzM4HTuojWzewElgO3EC8h3/k7quGtnYiIiJbTi3H\nw8zMXgjcBvwnfQfGEH+jfYhg+rfAy4eudv3yXfoRGKv1aEJqBeYBewGvBb4KPGJmZ5qZLszHkML/\n3fNHuj4iIkNJX1DDyMxeCfyIzS9KVgP/BB4DNgGzgR2BRTXyjjgzewZwXG7TA8BZwHXAmtz29cNZ\nLxkTpgIfB44ys+e7+6aRrpCIiEieguNhYma7Eq2t+WD3FuAjwO/dvbvGPtOAo4FXAC8BZgxDVZvx\n0sLz4939HyNSExkt3k90s8lrBbYB/gV4G3HBV3EM0ZL8xmGpnYiISJMUHA+fTwGTc88vAl7s7hvq\n7eDua4l+xr8zs3cAbyZal0faQbnHSxQYC7Dc3ZfU2H4PcJWZnQN8n7jIqzjZzL7k7jcNRwXHovSa\n2kjXYyDc/VLG+DmIyMQy6n6yH4/MrAN4cW5TF3BSo8C4yN3XuPsX3f2iQa9g/22de/zoiNVCxgx3\nXw+8Drgrt9mAU0emRiIiIrUpOB4eBwIduedXu/tYDirz08t1jVgtZExJF4NfLGw+diTqIiIiUo+6\nVQyP+YXnjwznwc1sBvBMYDtgLjFobhnwN3d/cEuKHMTqDQoz24Xo7rE90AYsAS5x98f72G97ok/s\nDsR5LU37PTyAumwH7A3sAsxKm1cADwJ/neBTmV1ceL6rmbW4e09/CjGzfYCnAQuIQX5L3P2HTezX\nBhwOLCR+ASkDjwM3D0b3IDPbHTgU2BbYCDwMXOvuw/p/vka99gD2B7Yi3pPriff6LcBt7l4ewer1\nycx2AJ5B9GGfTvx/ehS4wt1XDvKxdiEaNHYAWojPyqvc/b4BlLkn8frPJxoXuoG1wEPA3cAd7u4D\nrLqIDBZ3122Ib8CrAc/d/jBMxz0Y+APQWTh+/nYzMc2WNShncYP9690uTfsu2dJ9C3U4P58nt/1o\n4BIiyCmW0wl8BZhWo7ynAb+vs18Z+DmwXZOvcynV46vAvX2cWw/wZ+CYJsv+TmH/8/rx9/9MYd/f\nNPo79/O9dX6h7JOb3K+jxmuydY18+ffNpbntpxABXbGMlX0cd0/gh8SFYb2/zcPAe4C2LXg9jgT+\nVqfcbmLswEEp78JC+pkNym06b419ZwGfJC7KGr0nnwC+BRzSx9+4qVsTnx9NvVfSvq8EbmpwvK70\n/+kZ/Sjz0tz+S3LbDyMu3mp9JjhwDXB4P44zCXgv0e++r9dtJfGZ86+D8f9TN910G9htxCswEW7A\nswofhGuAWUN4PAM+1+BDvtbtUmB2nfKKX25NlZf2XbKl+xbq0OuLOm17Z5Pn+HdyATIx28b6JvZb\nAuzQxOv9xi04Rwf+B2jpo+ypwB2F/V7VRJ2eU3htHgbmDuJ77PxCnU5ucr8tCo6Jwaw/afBa1gyO\nif8LnyCCqGb/Lrc083fPHePDTb4PO4l+1wsL289sUHbTeQv7vQR4qp/vx5v6+Bs3dWvi86PP9wox\nM89F/Tz22UCpibIvze2zJG17B40bEfJ/w1c2cYytiIVv+vv6/Wqw/o/qpptuW35Tt4rhcT3RYtiS\nnk8Dvmtmr/WYkWKwfQN4U2FbJ9Hy8SjRonQwsUBDxdHA5WZ2lLs/NQR1GlRpzuj/TU+daF26lwiG\n9gd2zWU/GDgHOMXMjgF+TNal6I506yTmld43t99ONLfYSbHv/gbgVuJn69VEQLgjsB/R5aPiPUTQ\n9sF6Bbv7unSufwPa0+bzzOw6d7+31j5mNh/4Hln3lx7gte7+ZB/nMRy2Kzx3oJl6nU1MaVjZ50ay\nAHoXYOfiDmZmRMv76wtJG4jApdLvfzfiPVN5vfYGrjazQ9y94ewwZvZuYiaavB7i7/UQ0QXgAKL7\nxyQi4Cz+3xxUqU5fYPPuT48RvxQtB6YQXZD2pfcsOiPOzKYDlxF/k7yngGvT/QKim0W+7u8iPtNO\n7OfxTgS+lNt0C9Hau4n4HDmI7LWcBJxvZje6+911yjPgF8TfPW8ZMZ/9cuJiamYqfzfUxVFkdBnp\n6Hyi3IjV7YqtBI8SCyLsy+D93H1S4RhlIrCYVcjXSnxJryrk/1GNMtuJFqzK7eFc/msKaZXb/LTv\n9ul5sWvJ++rsV923UIfzC/tXWsV+C+xaI/8riSAo/zocnl5zB64G9q+x32IiWMsf6wV9vOaVKfY+\nk45RszWYuCj5ALCuUK/Dmvi7nlqo03XU+PmfCNSLLW4fG4L3c/HvcXKT+/1bYb976uRbksuT7wrx\nPWD7GvkX1tj2wcKxVqTXsb1G3p2BXxfy/4nG3Y32ZfPWxh8W37/pb/JKom9zpR75fc5scIyFzeZN\n+Z9LBOf5fS4Djqh1LkRw+SLiJ/3rC2nzyP5P5sv7GfX/79b6Oyzuz3sF+HYh/2rgrcCkQr6ZxK8v\nxVb7t/ZR/qW5vGvJPid+CexWI/8i4B+FY/y4QfnHFfLeTQw8rfleIn4dOh64APjpYP9f1U033fp/\nG/EKTJQb0QqysfChmb89SfRL/Bjwr8DULTjGNKLvWr7cM/rY5zB6B2tOH/3eqNMftI99+vUFWWP/\n82u8Zj+gwc+oxJLbtQLqi4DJDfZ7YbNfhCn//Ebl1ch/eOG90LD83H7FbgX/WyPPRwp5Lm70Gg3g\n/Vz8e/T59yQusm4v7FezDzW1u+N8ph/125veXSkeokbgVtjHiL63+WMe1yD/JYW85zZRp2JgPGjB\nMdEavKxYp2b//sA2DdLyZZ7fz/dK0//3iYHD+bzrgSP7KP/0wj5rqdNFLOW/tMbf4FwaXwhtQ+9u\nKhvrHYMYe1DJ1wXs3I/XarMLN9100234b5rKbZh4LHTweuJDtZY5wAuI/pEXAk+Z2RVm9tY020Qz\nTiJaUyr+6O7FqbOK9fob8B+Fze9q8ngj6VGihajRKPtvEi3jFZVR+q/3BssWu/tvgTtzmxY3qoi7\nP9aovBr5/wp8ObfpBDNr5qftNwP5EfPvNLPjK0/M7F+IZbwrngBO7OM1GhZm1k60+u5VSPp6k0Xc\nBHy0H4f8d7Kfqh14hddepKTK3Z1YyS8/U0nN/wtmtje93xd3Ed1kGpV/a6rXUHkLvecgvwR4R7N/\nf3dfNiS16p93Fp6f5e5XNdrB3c8lfkGqmEr/uq7cQjQieINjLCOC3orJRLeOWvIrQd7k7vc3WxF3\nr/f9ICLDSMHxMHL3nxI/b17ZRPZJxBRjXwPuM7O3pb5sjbyu8PzjTVbtS0QgVfECM5vT5L4j5Tzv\no7+2u3cCxS/WC9x9aRPl/yX3eOvUj3cw/Tr3uI3N+1duxt1XA68ifsqv+LaZ7Whmc4EfkfVrd+AN\nTZ7rYJhnZgsLt93M7Agz+3fgNuDlhX1+4O7XN1n+2d7kdG9mNgt4TW7T79z9mmb2TcHJeblNx5jZ\nlBpZi//XPpfeb335FkM3leNbCs8bBnyjjZlNBU7IbXqK6BLWjOKFU3/6HX/R3ZuZr/33hedPb2Kf\nrfpRDxEZJRQcDzN3v9HdnwkcRbRsNpyHN5lLtDRekOZp3Uxqecwv63yfu1/bZJ26gJ/mi6N+q8ho\ncWGT+YqD1v7c5H73FJ73+0vOwnQz27YYOLL5YKlii2pN7n4d0W+5YjYRFJ9P9O+u+G93/2N/6zwA\n/w3cX7jdTVyc/BebD5i7is2DuUZ+04+8RxIXlxU/68e+AFfkHrcSXY+KDs89rkz916fUivvTPjP2\nk5ltRXTbqPi7j71l3Q+h98C0Xzb7i0w619tym/ZNA/ua0ez/kzsKz+t9JuR/ddrJzN7eZPkiMkpo\nhOwIcfcrSF/CZvY0okX5IOILYn+yFsC8VxIjnWt92O5D75kQ/tbPKl1D/KRccRCbt5SMJsUvqnpW\nF57fWTNX3/v12bXFzFqAZxOzKhxCBLw1L2ZqmN1kPtz97DTrRmVJ8iMKWa4h+h6PRhuIWUb+o8nW\nOoAH3X1FP45xZOH5k+mCpFnF/3u19j0w9/hu799CFH/vR95mFQP4K2rmGt0OKjzfks+wp6XHJeJz\ntK/XYbU3v1ppcfGeep8JFwBn5J6fa2YnEAMN/+BjYDYgkYlOwfEo4O63Ea0e/wdgZjOJeUrfzeY/\n3b3NzL7p7jcUthdbMWpOM9RAMWgc7T8HNrvKXPcg7TepZq7EzA4n+s/u2yhfA832K684hZjObMfC\n9pXAa9y9WP+R0EO83k8Sdb0C+GE/A13o3eWnGdsXnven1bmWXl2MUv/p/N+r5pR6DRR/lRgMxW4/\ntw/BMYbaSHyGNb1apbt3FXq21fxMcPdrzewr9G5seHa6lc3sn8QvJ5fTxCqeIjL81K1iFHL3Ve5+\nPjFP5lk1shQHrUC2THFFseWzL8UviaZbMkfCAAaZDfrgNDN7HjH4aUsDY+jn/8UUYH66RtJ7+xp4\nNkROcXcr3Frdfa677+Hur3L3c7cgMIaYfaA/Bru//LTC88H+vzYY5haeD+qSysNkJD7Dhmqw6unE\nrzfrC9tLRIPH24gW5qVmdomZvbyJMSUiMkwUHI9iHs4kFq3Ie/YIVEdqSAMXv0/vxQiWEMv2Pp9Y\ntngWMUVTNXCkxqIV/TzuXGLav6ITzWyi/79u2Mq/BcZi0DJmBuKNR+mz+9PEAjUfAP7K5r9GQXwH\nLyb6oV9mZguGrZIiUpe6VYwN5xCzFFRsZ2Yd7r4ht63YUtTfn+lnFp6rX1xz3kbvVrsLgJOamLmg\n2cFCm8mt/FZcbQ5iNb+PElMCTlTF1umnuftgdjMY7P9rg6F4zsVW2LFg3H2GpSngPgd8zsymAYcS\nczkfQ/SNz38HPxP4o5kd2p+pIUVk8E30Fqaxotao8+JPhsV+mbv18xh79FGe1HZc7vEq4M1NTuk1\nkKnhzigc91p6z3ryH2b2zAGUP9YV+3DOq5lrC6Xp3vI/+e9aL28d/f2/2YziMteLhuAYQ21cf4a5\n+1p3/4u7n+Xui4klsD9KDFKt2A9440jUT0QyCo7Hhlr94or98W6h9/y3h/bzGMWp25qdf7ZZ4/Vn\n3vwX+JXuvq7J/bZoqjwzOwT4bG7TU8TsGG8ge41bgB+mrhcTUXFO41pTsQ1UfkDs7mlu5WYdMtiV\nYfNzHosXR8XPnP7+3fL/p8rEwjGjlrsvd/dPsfmUhi8aifqISEbB8diwZ+H52uICGOlnuPyXy25m\nVpwaqSYzayUCrGpx9H8apb4UfyZsdoqz0S7/U25TA4hSt4jX9vdAaaXEC+jdp/aN7v6gu/+JmGu4\nYnti6qiJ6C/0vhh75RAc46+5xyXgZc3slPqDv6LPjP3k7k8QF8gVh5rZQAaIFuX//w7V/92/07tf\n7kvqzeteZGb70Xue51vcfc1gVm4I/Zjer+/CEaqHiCQKjoeBmW1jZtsMoIjiz2yX1sn3w8Lz4rLQ\n9ZxO72Vn/+DuTza5b7OKI8kHe8W5kZLvJ1n8Wbee19Pkoh8F3yAG+FSc4+6/yj3/CL0val5kZmNh\nKfBBlfp55l+XQ8xssAPSHxSe/3uTgdwbqd1XfDCcV3j+hUGcASH//3dI/u+mX13yK0fOofac7rUU\n+9h/f1AqNQzStIv5X5ya6ZYlIkNIwfHwWEQsAf1ZM9u6z9w5ZvYy4LTC5uLsFRXfofeX2IvN7G11\n8lbKP4SYWSHvS/2pY5Puo3er0DFDcIyR8M/c44PM7OhGmc3sUGKAZb+Y2b/RuwX0RuD9+TzpS/bV\n9H4PfM7M8gtWTBSfoHd3pG/19bcpMrMFZvaCWmnufitwWW7THsAX+ijvacTgrKHyTWBZ7vmzgS82\nGyD3cQGfn0P4kDS4bCgUP3s+mT6j6jKz04Djc5vWEa/FiDCz08ys6X7uZvZ8ek8/2OxCRSIyRBQc\nD58pxJQ+D5vZL83sZWnJ15rMbJGZnQf8hN4rdt3A5i3EAKSfEd9T2HyOmf13WlgkX36rmZ1CLKec\n/6L7SfqJflClbh/5Vs3FZvZ/Znasme1eWF55LLUqF5cm/rmZvbiYycw6zOwM4GJiFP7yZg9gZvsA\nZ+c2rQVeVWtEe5rj+M25TW3EsuNDFcyMSu5+EzHYqWIacLGZfcnM6g6gM7NZZvZKM/sxMSXfGxoc\n5h1AfpW/t5vZD4rvXzMrpZbrS4mBtEMyB7G7ryfqm78oeBdx3ofX2sfMJpvZC83s5zReEfPy3ONp\nwO/M7CXpc6q4NPpAzuFy4Hu5TVOBP5vZm1L3r3zdZ5jZ54BzC8W8fwvn0x4sHwAeMLPvptd2aq1M\n6TP4DcTy73ljptVbZLzSVG7DbxJwQrphZvcADxLBUpn48nwasEONfR8GXtFoAQx3/5aZHQWclDaV\ngPcB7zCzvwJLiWmeDmHzUfy3sXkr9WA6h95L+74p3YouI+b+HAu+RcwesXt6Phf4tZk9QFzIbCR+\nhj6MuECCGJ1+GjG3aUNmNoX4paAjt/lUd6+7epi7/8zMvgacmjbtDnwNOLHJcxoX3P0zKVj7t7Sp\nhQho32Fm9xNLkD9F/J+cRbxOC/tR/j/N7AP0bjF+LfAqM7sGeIgIJA8iZiaA+PXkDIaoP7i7X2hm\n7wP+h2x+5mOAq81sKXAzsWJhB9EvfT+yObprzYpT8X/Ae4H29PyodKtloF05TicWytgvPZ+Zjv9f\nZnYtcXExHzg8V5+KC9z9qwM8/mCYQnSfej2xKt6dxMVW5cJoAbHIU3H6uV+5+0BXdBSRAVJwPDxW\nEMFvrZ/adqO5KYsuAt7S5Opnp6Rjvpvsi2oyjQPOK4Hjh7LFxd1/bGaHEcHBuODum1JL8V/IAiCA\nndKtaC0xIOuOJg9xDnGxVPFtdy/2d63lDOJCpDIo63VmdrG7T6hBeu7+VjO7mRismL/A2JnmFmJp\nOFeuu38xXcB8kuz/Wgu9LwIruomLwctrpA2aVKdHiIAyP5/2Anq/R/tT5hIzO5kI6jv6yD4g7r46\ndYH5Bb27X80lFtap58vUXj10pJWIrnV9Ta/3Y7JGDREZQepWMQzc/WaipeNZRCvTdUBPE7tuJL4g\nXuju/9rsssBpdab3EFMbXUjtlZkqbiV+ij1qOH6KTPU6jPgi+zvRijWmB6C4+x3AgcTPofVe67XA\nd4H93P2PzZRrZq+h92DMO4iWz2bqtJFYOCa/fO05ZrYlAwHHNHf/MhEIfx54pIld7iJ+qj/C3fv8\nJSVNx3UUMd90LWXi/+GR7v7dpio9QO7+E2Lw5ufp3Q+5lmXEYL6GgZm7/5gI8M4iuogspfccvYPG\n3VcCxxIt8Tc3yNpDdFU60t1PH8Cy8oPpeODjwFVsPktPUZmo/3Hu/mot/iEyOpj7eJ1+dnRLrU17\npNvWZC08q4lW31uB29Igq4Eeaybx5b0dMfBjLfGF+LdmA25pTppb+Cii1biDeJ0fAa5IfUJlhKUL\nhKcTv+TMIgKYlcC9xP+5voLJRmXvTlyULiAubh8BrnX3hwZa7wHUyYjz3RvYiujqsTbV7Vbgdh/l\nXwRmtiPxum5DfFauAB4l/l+N+Ep49aQZTPYmuuwsIF77bmLQ7D3ADSPcP1pEalBwLCIiIiKSqFuF\niIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqORUREREQSBcciIiIiIomCYxER\nERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqORUREREQSBcciIiIi\nIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqORUREREQS\nBcciIiIiIsmECo7NzNNt4Qgce3E69pLhPraIiIiINGdCBcciIiIiIo20jnQFhtmd6b5rRGshIiIi\nIqPShAqO3X2vka6DiIiIiIxe6lYhIiIiIpKMyeDYzOaZ2dvM7NdmdoeZrTGzdWZ2m5l9wcy2rbNf\nzQF5ZnZm2n6+mZXM7HQzu9bMVqbt+6d856fnZ5pZu5mdlY6/wcweN7MfmdkeW3A+083sZDP7iZnd\nko67wczuMbPzzGz3BvtWz8nMdjSzb5jZw2a2yczuN7PPm9mMPo6/j5l9K+XfmI5/lZmdamaT+ns+\nIiIiImPVWO1W8UHgvelxN7AamAksSrcTzezZ7n5zP8s14BfA8UAPsKZOvsnAJcAzgE5gI7AV8Grg\nxWb2fHe/vB/HPQk4Jz3uAVYRFy67pttrzewEd7+oQRlPB74FzEn1LgELidfpaDM7wt0362ttZqcD\n/0t2obQWmAYckW6vMrPj3H19P85HREREZEwaky3HwIPAh4H9gA53n0sErAcDfyIC1R+amfWz3JcC\nzwPeBsxw99nANsB9hXynpWO/AZjm7jOBA4AbgCnAT8xsdj+Ouxz4FHAoMCWdTzsR6P8AmJrOZ2qD\nMs4HbgL2dfcZRID7JmAT8bq8pbiDmZ1ABOXrgH8HtnL36ekcngfcDSwGvtiPcxEREREZs8zdR7oO\ng8rMJhNB6tOAxe5+WS6tcrI7u/uS3PYzgY+np2919/PqlH0+0coLcKK7/6CQPg+4A5gLfMzd/zOX\ntphobX7A3Rf243wMuBB4NnCyu3+nkF45p1uBg9x9UyH9HOB04BJ3f1ZuewtwL7AT8Dx3/1ONY+8K\n3Ay0ATu6+9Jm6y0iIiIyFo3VluO6UnD45/T0yH7u/iTRNaEvDwA/rHHs5cDX09OX9/PYNXlcvfwu\nPW10Pl8oBsbJr9L9PoXti4nA+JZagXE69r3ANUT3m8VNVllERERkzBqrfY4xs72IFtGjiL6104g+\nw3k1B+Y1cJ27dzeR7zKv3+R+GdHlYx8za3P3zmYObGbbA+8gWoh3Baaz+cVLo/P5e53tj6T7YjeP\nI9L97mb2WINyZ6b7HRrkERERERkXxmRwbGavBr4LVGZSKBOD2Cotp9OIfrqN+ujW8kST+R5pIq2F\nCEiX9VWYmR0N/Jaod8UqYqAfQAcwg8bnU2/wYKWM4t96QbqfTPSr7suUJvKIiIiIjGljrluFmW0F\nfIMIjH9MDDZrd/fZ7j7f3eeTDSDr74C8nsGraXPSVGnfJwLji4iW8A53n5U7n/dUsg/ioSt/+1+7\nuzVxO3MQjy0iIiIyKo3FluPnE4HkbcBr3b1cI08zLaED0ah7QyWtB3iqibIOB7YHVgDH15kybSjO\np9KiveMQlC0iIiIyJo25lmMikAS4uVZgnGZ3eFZx+yA7uom0W5rsb1w5n7sazCX87KZr1ry/pvv9\nzGy7IShfREREZMwZi8HxqnS/T515jN9CDGgbSgvN7DXFjWY2B/i39PSnTZZVOZ/dzay9RpnPAY7Z\nolo2djHwENE3+r8bZeznnM0iIiIiY9ZYDI4vApyYmuxLZjYLwMxmmNn7gS8TU7INpVXAN8zsdWbW\nmo6/H9kCJI8DX2myrKuA9cTcyN81swWpvA4zeyPwc4bgfNJqeacTr+VrzOxXlWWy0/HbzOwZZvY/\nwP2DfXwRERGR0WjMBcfufidwdnp6OvCUmT1F9O/9HNEi+rUhrsZXgVuIgXRrzWwV8A9icOB64BXu\n3kx/Y9x9JfCh9PQVwKNmtpJYEvubwD3AWYNb/eqx/x+xil4nsWT2jWa23syeJM7jr8RgwJn1SxER\nEREZP8ZccAzg7u8hui/cSEzf1pIevxs4DmhmruKB2EQsivEJYkGQNmIauAuAA9398v4U5u5fIpau\nrrQitxIr7X2cmI+43jRtA+bu3wb2JC44biUGEs4gWqsvTXXYc6iOLyIiIjKajLvlo4dSbvnoszS1\nmYiIiMj4MyZbjkVEREREhoKCYxERERGRRMGxiIiIiEii4FhEREREJNGAPBERERGRRC3HIiIiIiKJ\ngmMRERERkUTBsYiIiIhIouBYRERERCRpHekKiIiMR2Z2P7EU+5IRroqIyFi0EFjt7jsP94HHbXD8\np3PPcIByr9k4DICSxb2VLJdi+SyYGUVe2ZRLq+Sr7N+rzEpa9X7zhvr8YSrplhr0rVTKpRXLynb0\nQrH54xTPNb9fa0sLAPsf/57NT1ZEBmpGR0fHnEWLFs0Z6YqIiIw1t99+Oxs2bBiRY4/b4Nir9/kA\nOJTTfcnz+b33jtmDakCZBcfk0nrflzwXtKbAPMtTpqhXEJ7SK1taagTom9cOvFwsK0stVy4IaszY\np1n8pBYzuxQ42t2H9KLJzBYC9wPfcfeTh/JYI2TJokWL5lx//fUjXQ8RkTHnoIMO4oYbblgyEsdW\nn2MRERERkWTcthyLyBZ7AzBlpCsxHtzyyCoWfvB3I10NEZERseSzx410FbbIuA2Oq32Aa3SPqGh2\ndcBa/Y+bydNor0r+UinfeN+7b3KpRp/jUrEfB1Au9JPO9zmudveo8RtBE6clE5C7PzjSdRARERkp\n6lYhMgGY2clm9nMzu8/MNpjZajO7ysxOrJH3UjPzwrbFZuZmdqaZHWpmvzOzFWnbwpRnSbrNNLNz\nzewRM9toZreZ2TutmavMKGcPM/usmV1nZk+Y2SYze8DMzjOz7Wvkz9dt/1S3lWa23swuM7Mj6hyn\n1czeZmYYn10iAAAgAElEQVTXpNdjvZndaGanW63RsyIiMiGM25Zjb+mJBz3ZKVq5PT3oBqBsXVl+\nerci57/Hy9U0y/1Lzfy9vv8rzbVp91Juz8pjz38HV8to7fU8HpdS/s2TWjarV3ZeWMxIUfaO2L8l\nSyuVumuciYxTXwVuBS4HlgJzgRcA3zOzPd39Y02WczjwIeBK4FvAPKAzl94GXATMAi5Iz18G/C+w\nJ/D2Jo7xUuBU4BLg6lT+3sCbgReZ2cHu/kiN/Q4G/h34K/B/wI7p2Beb2f7ufmclo5lNAn4DPBe4\nE/ghsBE4BjgHOAx4fRN1FRGRcWbcBsci0ss+7n5vfoOZtQF/AD5oZl+rE3AWPQc41d2/Xid9AXBf\nOt6mdJyPA38H3mZmP3b3y/s4xveAL1b2z9X3Oam+HwVOq7HfccAp7n5+bp+3Al8D3gW8LZf3I0Rg\nfC7wbnfvSflbgPOAN5rZz9z9133UFTOrNx3FXn3tKyIio884/umwBJTwUk/11t2yPt020N2ygbL1\nVG8OvW6YbXazBreK3tvSrVSCUglzqjfKZHPKVXcupVscz8luxW09lKq3TTaJTTaJjd7GRm+jszyp\neiuXS5TLJVrcaHGj1DW5evOuDryrYzj+GDLCioFx2tYJfJm4SD62yaJuahAYV3woH9i6+wrgk+np\nKU3U9ZFiYJy2X0i0fj+3zq5X5QPj5FtAN3BoZUPqMvEO4DHgjEpgnI7RA7yX+Bh4XV91FRGR8Uct\nxyITgJntCHyACIJ3BIpXRds1WdS1faR3E10hii5N9wf0dYDUN/l1wMnA04HZQEsuS2eN3QCuK25w\n9y4zW5bKqNgDmAPcDXy0TlfoDcCivuqajnFQre2pRfnAZsoQEZHRQ8GxyDhnZrsQQe1s4ArgQmAV\n0EMsz3kSMLnJ4h7rI315viW2xn4zmzjGF4B3E32j/wQ8QgSrEAHzTnX2W1lneze9g+u56X534OMN\n6jGtibqKiMg4M26D41JaU9lzg9MsDUbr6Z4aG3LrLre0VAanVaZYy3eVSMs51xp0V9BrejirrJAX\n+5fLWczQ1RXHa2+bmstfWIq61vRwlQF55dzgvvQL9KSONQBMm5qd85TWJwFoLUVs0t2TxSbrOufX\nPQ8ZV95DBISnFLsdmNlriOC4WX3NfzjPzFpqBMiVN9uqRjub2dbAO4FbgCPcfU2N+g5UpQ6/dPeX\nDkJ5IiIyjozb4FhEqnZL9z+vkXb0IB+rFTiCaKHOW5zub+xj/12IAQMX1giMt0/pA3UH0cr8DDOb\n5O5dfe2wpfbZbibXj9FJ8EVEJqpxGxxXWo5bSk9Ut7W3PwTApNSItWZDNmVqJ5OAbDq0/DSnlVbk\nJqdpzVjvKdbKuVblSgtzfqEPL0ytWutw5VREa66saW2rI23KEgCe2pDFFN2T49fo7baOdR1ayk9V\n01o79232TGRsW5LuFxPTlwFgZs8lpkcbbJ8xs2Nzs1XMIWaYAPh2H/suSff/km+BNrNpwDcYhM8s\nd+82s3OAjwFfMrP3uPuGfB4zWwDMdvfbBno8EREZW8ZtcCwiVV8hZon4qZn9DHgU2Ad4HvAT4FWD\neKylRP/lW8zs/wGTgJcTU7x9pa9p3Nz9MTO7AHg1cJOZXUj0U/5XYh7im4D9B6GenyQG+51KzJ38\nF6Jv89ZEX+QjieneFByLiEww43gqNxEBcPebicUtribmAj4NmEEstvG1QT5cJ/BsYtDfq4G3En18\n3wWc3mQZbwI+Tcyo8XZi6rbfEt01GvZZblbqSnEC8AZiEZAXElO4PY/4XPwY8IPBOJaIiIwt47bl\nuFwZ1NazvLqta+NNAMyeHQPzp049uJq2fGV0ZezxKQC0tGWrx5VSl4vutNqe95qguDKALwbD57tj\neEor90QXiK4NucGBhS4XAGWLGapKaVU7S3WJxzG+qTQpulDMnJt1F+mYEud4530bAVi6NKv7bguj\nrG23WR/n1511x+hePwuZGNz9auBZdZKtkHdxjf0vLeZrcKxVRFDbcDU8d19Sq0x3X0+02n6kxm79\nrpu7L6yz3YkFR77XqJ4iIjKxqOVYRERERCQZty3HPS3RUtpSztY6ePyxrQBYcmdMa3bgwX+vps2f\nEy2x6zpnADB5+tpqWkdLbFu1JlaDXblu86laKwPsenqyVuWecrnyAIC2SdlUq5UW5u5NuSnj2qKu\n5XTJ0kK2SNiU9piSbdac+6N+0+6pppU8ythv5zYA9t8zWwW4MiNddzQqs2ZVtgbDpvU11y4QERER\nmbDUciwiIiIikozbluMWb497dqhua2+PfrvLl8f92rVZf+RpHfcCMKPN037rqmnT2+cA0NkZLc8r\n1kyvpvV0Rz9i92gVbmmZVE1rbY3HpbZoOZ5Uyvoce+qH3NOZtTSXUmty65RY6GvmzEeraTNnPABA\nR2u0ehsrqmlm0Yd68qS07kJrlnbHkm0BWNd9BABTJ+1DtuPWiAyWen17RURExhK1HIuIiIiIJAqO\nRURERESS8dutgujmMLk1m7ps/q7RrWGvfdJiWBt6qmnlzkhrmRTXC+tWzqumPfVkdE1Y0xUD3rq6\ns/1aUzeKltZIK/Va5S6OXU6D4jq9LbdfTLfW1pZ17WifHN0htpkfU7lOmfFYrqylUX4a3NeS+9O1\ntMbgww0WaSueOKSaduEfo8vFjnvFOe+3R3a8HjoRERERkYxajkVEREREknHbcjx7bgywmzvrluq2\n6ZOXAdDiqeV4Zja1Wmo4ZsP6aAle/nh23dA6JQb1zZw7G4COzg3VtK7ueAm7uypTuU2uprWkAXIt\nllqcO7OBfD4ppo6bv2O24Jd3RuvwA3c9HnWflw3g22q7eFyaGvnLntXv7ttiwOCDyxYB0DZzt2ra\ng09eBcC2G5ekc7+pmlayXRERERGRjFqORURERESScdtyvN12/wBgWmtusYzuaBXuaonTXseMatrt\nt6YW4LVrAFi4+8pq2rSZ1wMwpf322FDOFucol6M1eGNnmjJt0/bVtEeWR4vxytVxP3NGtqzzpGlR\n/p2PPF7dds3VD8d+aRnoA/bMrl1e/LLUf3lTLCm9cuW+1bTr/hHlP3h/9Cd++gHZ0tLtFq3ly5fF\nsUuLslbvbjSVm4iIiEieWo5FRERERBIFxyIiIiIiybjtVrFsRXRv2NAxrbqt1WcCsLYzrgkeeCLr\nAnH136N7w6yOmD7tqe5slbn77o3Hu2wXXRMOPTBb1W7e3Mg/edoSAJz51bQH7+kAYPb8qMPc7adU\n0264YS0A112fTa02tSOmVnvJy6N+u2yfrbZ3351zI/8N8Xy7HWZW0+ZOfyjOb7v7AChnVWfv7aKM\nedvGioEbuveoppXan46IiIiIZNRyLCKjipm908xuM7MNZuZm9u6RrpOIiEwc47blePnjhwOworSx\nus3SIhxljxbdtZuyBUJ22mMWAPNnPw2A7q411bRVG24G4Ju/iMF9F12TLQLyvKOjrAMOjOuM9unZ\n9caxx0YL8+xpMdCus5yllXaPuuyxY1a/bRfEaiHlNTHg79EHsxbq+2+L+zYiz1Zb3ZqV1RWD7jrK\nUa95W2VT1JVL0WrdMS1aibtL+2T7kS10IjIamNmrgf8FbgTOBjYB14xopUREZEIZt8GxiIxJL6zc\nu/ujI1qTQXDLI6v6ziQiIqOKulWIyGiyLcB4CIxFRGRsGrctx55Wpev03ClarDJnRHeFWR1WTZo7\nJQa4eXdcL3S3zammHXbEVgBM32ZnAJY9fGNWZEsM0pveEQPfOmZ0ZnUorQagXI7jtrVk1yJ77h35\nVqS5kAHu/2dsW//kegDap2cr5B18aBpMGEUybVo21/KaVdGNYvLsGHTXOi2r+/pNR8Xr0LZjnFe5\no5rW1pMN+BMZSWZ2JvDx3PNqnyd3t/T8MuDVwH8CzwfmA29y9/PTPguAjwLHEUH2KuAK4FPufn2N\nY84EzgJeDswDlgDnAb8C7gW+4+4nD+qJiojIqDdug2MRGVMuTfcnAzsRQWvRHKL/8VrgF0AZWAZg\nZjsDVxJB8V+AHwE7AK8AjjOzl7n7bysFmVl7yncg0b/5B8BM4CPAM/tTcTPbLPBO9upPOSIiMjqM\n2+DYqvfZoDvc0rY0oC43QK47pXkaA+eeDcibPSVack84KlpaZ8zcqpo2c85SANpaYuU582wFOvdo\nVfbUQLsua+zl4ktj429/vbq6bd7UqOuzj4x6bbtzrmV7mxi4VypFS3PXxixt3apoVX5y1XYArFy3\nZzWtpXVRnF935SXIBvmVPXssMpLc/VLgUjNbDOzk7mfWyLYv8D3gjV75z5X5GhEYf9TdP1XZaGZf\nAS4HvmNmO7n72pT0fiIwvgB4rbt7yv8p4IbBOi8RERl71OdYRMaKTuB9xcDYzLYHngM8CHwun+bu\nVxOtyHOAl+aSTiJanj9UCYxT/oeIWTKa5u4H1boBd/SnHBERGR3Gbctx5fvOc63DRvTN9dSa3ENL\nLi2+b1tbo+W3Y2rWomuT7wfgCY+FQjZtzPrqzixPBqA0KVqQvZSl9WycDcBTj0Xf4SuuzJqOf/u7\nOM6jK7MW4O6do0X7hlsj/+R52Z9n0pTI1zE5WpAnt2WtvrNnxjmufHI6AF3rsj7Hk+b2pHOO1m98\ncjWt7Lo2kjFliXv6T9jbAen+CnfvqpH+F+DElO+7ZjYD2BV4yN2X1Mh/5WBUVkRExiZFRyIyVjxW\nZ3tlucilddIr22el+xnpflmd/PW2i4jIBKDgWETGCq+zvTKZ8Pw66QsK+So/C21TJ3+97SIiMgGM\n324Vle/R3FRulW2epnQrkQ26mznjSQDmbPUQAB0zsuuG2+6I7hf3LI0V5RbulH1H77VTGohXijwr\nVm5bTXv0sRgY99jS6Arx8KPZqnYLF8T38z47ZfUrl6PrxLqn4vndN+S6Vq6Jx3NmRTeJKVNzg+na\nokFt6dLYv7M7q8O0OREXVKa0y18NlbxerCEyplTmVvwXM2utMVjvmHR/A4C7rzaz+4CFZrawRteK\nfxmsiu2z3cy+M4mIyKiilmMRGdPc/WHgz8BC4N35NDM7DHgt8BTwy1zSd4nPv8+YmeXy71AsQ0RE\nJpZx23Kc6ak+cotW3raOdQBsP395NW3B3AcAaG9Lg+5yv+A+/ZAo47Ap0TpsXdkgumWPRQvuYw/v\nBMCGNbtW09anH29Xr4iW3SmTVlbTttkj6jKpOxvA98TKOObSNfFnuX9ZezVtRXfknzEr6lAiS1u7\nLurQWo6ytt0mG5NUGZ/k5bTYSEmtxTIunQpcBfy3mT0HuI5snuMycIrn52eMWS1OIBYV2dPMLiT6\nLr+SmPrthLSfiIhMMGo5FpExz93vAw4m5jveE3gfsYreH4Ej3f3XhfwbiO4W5xB9lc9Izz8NfCZl\nW42IiEw447/l2PKNP9HiW06ttavXZUs9l1K2qe0x1VlLx6pq2qZUxroVkda1LuvTe+GfowX3gQfj\nOuM5Rz9RTZvcE9OctvVEg9UuueFCHVNjP5ucLR/dvXwuALffGH2bd991/2rany67FIBJ02K/e+/J\nBuZvOz3+jMcfsxsAPaXZWZml6H5ZaTA2zy0Z7VkLuMho4O6L62zv883q7o8Ap/XjWCuBd6ZblZm9\nJT28vdmyRERk/FDLsYhMSGa2bY1tOwIfA7qB3wx7pUREZMSN/5ZjEZHafm5mk4DrgZXEgL4XAlOI\nlfMeHcG6iYjICBm3wXH2G6zntkWXgs4NMaht6Zr9qmmPE9OutZSim2Fbe9ao/ugTse2mW+4CYO+9\ndq+mbepeC8Dq7lhUy31tNa0lLYi3YNt4mWfNzVWwbUraP6vDLrOOAOCx9SsAeHBZNmCwbdrWkfZk\nmueNrHvE/otiurZNa2LtgieWb1VNm7NTPJ6UVsMzrYonUvE94PXAy4jBeGuBvwHnuvsvRrJiIiIy\ncsZtcCwi0oi7fwX4ykjXQ0RERpfxGxynqUtzU5jiXh2VFnet66pp3an7dbfHYLZN3dl+LR3RSvvU\nipjSbe26m6tpbe2xeEh5bUpb1VFN231BTL82ZWbs392yYzVt9aYDIn/nntVtDyyLpubb7nkYgH/e\nfnc1bc2aGCC436JotT72Rc+qps31+6Oe5dh/1k7ZdHI96ZSzAXm5lnQNyBMRERHpRb+xi4iIiIgk\nCo5FRERERJLx262igVIprglKns0x7GmAW5mYF7i9PRuovu28BwGYe0LMLTxrxrJq2oZ1UcZ9N8f+\nLa3Z9UbHvBgMt3p9dIXYuDGbt7iLSJsxfWZ12wPX/hWAX/zpzwBM7phWTdt3z10AOPHlzwdg++nZ\nn+7R+6I+U6dHma1T51TTNpZijuXJ5dSdwrVCnoiIiEg9ajkWEREREUnGbcuxe0+6zw1Aqw7Sq7Qc\nZ4PnSi3RwjpjZrQSbz33nmrajLZ7I8+2MY1a2bNV9zp7ouX49afEQL5yz7xq2vKnYpq2DeVYGs/a\nspZqS9cl5Y3ZKn177rIDAAfsuxcA/7w1W6Br5rRomZ4+PU1HR7bfnO2jZdrT+XWR1a81HcfoPRix\nUgsRERERyajlWEREREQkGbctx9UW0lxLqafW01IpFgHxctb62tYeC31MmxFTs7XkpnnrLM9Ij6J/\ncLmUvWzruyZHmRYLcaxdmy0Q0t0dfYAtrddhPS3VtJZStO529mR12GHBNgCc+poXAfDAA9kCIdtv\nH2mW5mTbWM6uaya3pfqlVvJJbKimVU8/tSo76nMsIiIiUo9ajkVEREREEgXHIjJqmNlCM3MzO7/J\n/Cen/CcPYh0WpzLPHKwyRURk7Bi33Sq8OnPZ5gPyPA2oK+UGrnVtjAF1jz1yIACPty7K9muJLhal\n1sjf050NrOvemAb19VQG97Xk9quUX7kGyQbAlVO9el2d9MQKdztuPR2AnRdk3SrK5Rhg2F3phpHr\nHVGuDLpL51eyJgfaqYeFiIiISC/jNjgWkQnhl8A1wNKRroiIiIwP4zY4LpfLve4hN5VbKd3n8neX\n2wHw8tS496xNt5xadK2zK+49v2e0FFdalUu2abO6lHKtyRlPx8m2mKWW6XIsRNLTk0tLZZi1VLdk\n+/VuAq7VWl4rzbUgiIxx7r4KWDXS9RARkfFDfY5FZFQys73M7FdmtsLM1pnZlWb2nEKemn2OzWxJ\nus0wsy+kx135fsRmto2ZfdPMlpnZBjO7ycxOGp6zExGR0Wrcthx7ajH2cr5pttIROa4JesgtylFJ\na1nbKytAqVxZNKTSp7c7S2zZmAqIPE57tp9XWqjL6Rj5VlzrXSeyRTy80jps2bVLtcXYWzbbD++q\nPIh/80m5BUuKaVoCREaxnYG/Av8Evg4sAF4F/MHMXuvuP26ijDbgL8Ac4EJgNXA/gJnNA64GdgGu\nTLcFwNdSXhERmaDGbXAsImPaUcDn3f39lQ1mdi4RMH/NzP7g7qv7KGMBcBtwtLuvK6R9mgiMz3b3\nM2oco2lmdn2dpL36U46IiIwO6lYhIqPRKuAT+Q3ufh3wA2AW8JImy3lvMTA2s0nA64A1wJl1jiEi\nIhPUuG059jSarXe3isqAvJSn1JnbI6VVu0JknQ5K1anSek+ZFvlT14zKgnxWY5BbtZtEbr/K8Uq1\nBs+lQX75bhhWOUx06ch3l6h026huqzHoLrvPF6lrIxm1bnD3NTW2XwqcBBwAfKePMjYCN9fYvhcw\nBbgiDeird4ymuPtBtbanFuUDmy1HRERGB0VHIjIaLauz/bF0P7OJMh732lOyVPbt6xgiIjIBjd+W\n4xqLgFQeVQbr5adkq0zvVkpNtF7KLeZRGShXbU3Oteh672FtlenYSKUV65BlLFQqv1etRTyyE+p9\nTzZdXaXluNZ0bdVtvcrWVG4yam1TZ/v8dN/M9G313uCVffs6hoiITEBqORaR0ehAM5teY/vidH/j\nAMq+A1gP7G9mtVqgF9fYJiIiE4SCYxEZjWYC/5HfYGYHEwPpVhEr420Rd+8iBt1NpzAgL3cMERGZ\noMZtt4qWls3j/s16K9jmaZUuDaVSPq2ybfPBetXpirMHNQ6weTeJakouqXLs4qp2wQv5a9Shkpjr\n6lH8XTlfdGtLrZX7REaFy4E3m9lhwFVk8xyXgLc2MY1bXz4MHAu8OwXElXmOXwX8HnjxAMsXEZEx\natwGxyIypt0PnAp8Nt1PBm4APuHufxpo4e6+3MyOJOY7fhFwMHAncBqwhMEJjhfefvvtHHRQzcks\nRESkgdtvvx1g4Ugc22oP5hYRkYEws03EvIz/GOm6yIRWWYzmjhGthUxkW/oeXAisdvedB7c6fVPL\nsYjI0LgF6s+DLDIcKis46n0oI2Usvgc1IE9EREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqO\nRUREREQSTeUmIiIiIpKo5VhEREREJFFwLCIiIiKSKDgWEREREUkUHIuIiIiIJAqORUREREQSBcci\nIiIiIomCYxERERGRRMGxiIiIiEii4FhEpAlmtr2ZfcvMHjWzTWa2xMzONrPZI1GOTEyD8f5J+3id\n22NDWX8Z+8zs5WZ2jpldYWar0/vm+1tY1qj8PNQKeSIifTCzXYGrga2BXwN3AIcCxwB3Ake6+5PD\nVY5MTIP4PlwCzALOrpG81t0/P1h1lvHHzG4Cng6sBR4G9gJ+4O4n9rOcUft52DoSBxURGWO+QnyA\nv9Pdz6lsNLMvAGcAnwJOHcZyZGIazPfPSnc/c9BrKBPBGURQfA9wNHDJFpYzaj8P1XIsItJAat24\nB1gC7Oru5VzadGApYMDW7r5uqMuRiWkw3z+p5Rh3XzhE1ZUJwswWE8Fxv1qOR/vnofoci4g0dky6\nvzD/AQ7g7muAq4ApwDOGqRyZmAb7/TPZzE40sw+b2bvM7BgzaxnE+oo0Mqo/DxUci4g0tme6v6tO\n+t3pfo9hKkcmpsF+/8wHvkf8dH028BfgbjM7eotrKNK8Uf15qOBYRKSxmel+VZ30yvZZw1SOTEyD\n+f75NnAsESBPBfYFvg4sBP5gZk/f8mqKNGVUfx5qQJ6IiMgE4u5nFTbdApxqZmuB9wJnAi8Z7nqJ\njBZqORYRaazSgjGzTnpl+8phKkcmpuF4/3wt3R81gDJEmjGqPw8VHIuINHZnuq/X9233dF+v79xg\nlyMT03C8f55I91MHUIZIM0b156GCYxGRxipzeD7HzHp9ZqYph44E1gPXDFM5MjENx/unMjPAfQMo\nQ6QZo/rzUMGxiEgD7n4vcCExWOntheSziFa271Xm4jSzSWa2V5rHc4vLEckbrPehmS0ys81ahs1s\nIXBuerpFSwGLFI3Vz0MtAiIi0ocay5zeDhxGzNV5F3BEZZnTFGTcDzxQXGShP+WIFA3G+9DMziQG\n3V0OPACsAXYFjgPagd8DL3H3zmE4JRmDzOwE4IT0dD7wXOLXhivStuXu/r6UdyFj8PNQwbGISBPM\nbAfgE8DzgLnECk6/BM5y96dy+RZS58ugP+WI1DLQ92Gax/hU4ACyqdxWAjcR8x5/zxUYSAPpAuvj\nDbJU33Nj9fNQwbGIiIiISKI+xyIiIiIiiYJjEREREZFEwfEAmZmn28KRrouIiIiIDIyCYxERERGR\nRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDjug5mVzOwdZvYPM9tgZk+Y2W/M7PAm9j3AzL5vZg+Z\n2SYzW25mfzKzl/WxX4uZvdvMbs4d87dmdmRK1yBAERERkSGgRUAaMLNW4GfA8WlTN7AWmJUevwr4\neUrb2d2X5Pb9N+CrZBcgK4HpQEt6/n3gZHfvKRxzErGM4vPrHPPVqU6bHVNEREREBkYtx419gAiM\ny8D7gZnuPhvYBbgI+FatnczsCLLA+GfADmm/WcBHAQdOBD5UY/ePEoFxD/BuYEbadyHwR+D/Bunc\nRERERKRALcd1mNlUYo3v6cQa32cW0icDNwBPS5uqrbhmdjHwLOAq4OgarcOfJgLjtcB27r46bZ+e\njjkV+Ii7f7qw3yTg78DTi8cUERERkYFTy3F9zyEC403AF4uJ7r4J+Hxxu5nNAY5JTz9TDIyT/wI2\nAtOAFxSOOTWlfanGMbuAL/TrLERERESkaQqO6zsw3d/k7qvq5LmsxrYDACO6TtRKJ5V3feE4lX0r\nx1xb55hX1K2xiIiIiAyIguP6tkr3jzbI80iD/VY1CHABHi7kB5iX7pc22K9RfURERERkABQcD53J\nI10BEREREekfBcf1PZHut22Qp1ZaZb8OM9uqRnrF9oX8AMvT/YIG+zVKExEREZEBUHBc3w3pfn8z\nm1Enz9E1tt1I9DeGbGBeL2Y2EziocJzKvpVjTqtzzGfW2S4iIiIiA6TguL4LgdVE94h3FRPNrA14\nb3G7u68ALklPP2BmtV7jDwDtxFRuvy8cc11Ke3uNY7YCZ/TrLERERESkaQqO63D3dcDn0tOPm9l7\nzKwDIC3b/Etghzq7f4xYOORA4AIz2z7tN83MPgx8MOX7bGWO43TMNWTTxv1nWra6cswdiQVFdh6c\nMxQRERGRIi0C0sAAl49+K/AV4gLEieWjZ5AtH/0D4KQaC4S0Ab8h5jyudcz88tHbunujmS1ERERE\npB/UctyAu3cDLwPeCdxMBKc9wO+Ile9+0WDfrwOHAD8kpmabBqwC/gy8wt1PrLVAiLt3AscRXTZu\nScerHHMxcHEu+8qBnaGIiIiI5KnleIwxs2OBi4AH3H3hCFdHREREZFxRy/HY8/50/+cRrYWIiIjI\nOKTgeJQxsxYz+5mZPS9N+VbZvreZ/Qx4LtAFfGnEKikiIiIyTqlbxSiTBgF25TatBlqBKel5GTjN\n3c8b7rqJiIiIjHcKjkcZMzPgVKKFeF9ga2AS8BhwOXC2u99QvwQRERER2VIKjkVEREREEvU5FhER\nERFJFByLiIiIiCQKjkVEREREEgXHIiIiIiJJ60hXQERkPDKz+4EZwJIRroqIyFi0EFjt7jsP94HH\nbX+6PK4AACAASURBVHD80pe8yAFaWydVt5VKMTNHd896AFau3FhNu+bafwIwbXZMJ7z7PtnfYumK\n+wF4/KllAGzs6qmmTW6fHGVujEb4ebOy/WZN2wGArp5NAJTLm6pplorozE0W0t2W/hytUVZ59bpq\nWrmnJ5VRBsDTPUBPSusux/TI3d1Z/SrFm1s8b2mppm2/9TYA3HX1ZYaIDLYZHR0dcxYtWjRnpCsi\nIjLW3H777WzYsGFEjj1ug+PnPP+5ALS0ZHHf6rXLAbjr7giEd9wzC2Svu+0WALbedmsAnvGMw6tp\nf/7LkwA8sSn2b+3KAswNGyMQbWlvB2Duwh2qaXvufRAAk3sikO3qyoLxDalHy4ZccLypK/KVU3Br\n5e5qWmdnJwDd3bGtp9yTS4ugeGNPSuvK9iPl97T/6uVP5vbL6iMig27JokWL5lx//fUjXQ8RkTHn\noIMO4oYbblgyEsdWn2MRGVPMbImZLRnpeoiIyPik4FhEREREJBm33SpKbRH3t7Rm/RaefDj6DF9y\n5cUAPPsFL6imTZoW+Vvb437u3LnVtBc998UA/P4PvwHgzrvvyw6U+h/3dEf/4GktWXeHrabFfXtP\ndMPYUO6opq0opX7FnvWJ7uiKP4d3R/eKTu+qprV2RbeIrnTfk+tX3Jq6TlS7UOTSNq1cCcDjDz0E\nwJqlS6tpk+fOQ0SGzi2PrGLhB3830tUQGdOWfPa4ka6CTDBqORYRERERScZty/GU9jYAWlqzWR1m\nzGhP99Fae9c9t1TTNnatBuDRRx8E4O477qymHXHIwQDsv/c+ADz++CPVtGlTZgIwc0qUvVXnU9W0\nbTc8EcebFoPVn+zO6tJVeei5iSLKaaBfOa5ZnCzNy+mxR56SZy3iJUt/Ros8a9etr6YtveNuANY/\nHq3m5OpQys26ITKamJkBbwdOA3YFngR+CXykTv7JwBnA61L+buAfwDnu/pM65b8TeCuwS6H8fwC4\n+8LBPCcRERkbxm1wLCJj2tlE8LoUOA/oAo4HDgPagM5KRjNrA/4EHA3cAXwZmAK8HPixme3v7h8u\nlP9lIvB+NJXfCbwYOBSYlI7XFDOrNx3FXs2WISIio8e4DY43rV8LgFnWwjqtYyoA224V8/s+suyJ\nbIfUirphY+z31LLHq0mtqQW3Z2NMfbb/vttV0479lyMBaO+KPG2bsu/Uqan/ccmizMcfy/r7bjM3\nyphayvocb0qdXLpb4oGVs14vmywe96S+yt2lbDq5rtTHuGVTzKN89z9uqqatXxYtxpPS/MYtudbo\ntlwZIqOFmR1BBMb3Aoe6+4q0/SPAJcAC4IHcLu8lAuM/AC929+6U/yzgWuBDZvZbd786bX8mERjf\nBRzm7ivT9g8DFwHbFsoXEZEJRH2ORWS0OSXdf6oSGAO4+0bgQzXyv5FY7+Y9lcA45X8c+GR6+uZc\n/pNy5a/M5e+sU35D7n5QrRvRii0iImOMgmMRGW0OTPeX1Ui7Eqh2ljez6cBuwKPuXisY/Uu6PyC3\nrfL4yhr5ryH6K4uIyAQ1brtVzJgS06aZZd0IZqcBebvssBsAd9+V/XI6rSVeivb2WD567VPVBiuW\nPfxwr22z2nOD/FK3je3mR1cN25QtET25FN+xXT3RPbJ860PVtFnTo4tHa2vWtWFN6uaw3qKrxaTO\n7DhOPO5JC0K35LqLtKRlsR+48zYA1i3LBgy2p/NyT/FEbsVASro2klFpZrpfVkxw924zW14j79Ji\n3sL2WU2W32NmTxa3i4jIxKHoSERGm1Xpfptigpm1AvNq5J1fp6wFhXwAqxuU3wLMLW4XEZGJY9y2\nHFtPtNpa7hS9J1pYZ02JqdX+P3t3Hl/XVd77//OcSZNl2bIt27FxlIQMDoEAhhBCIElpCTSFAoVL\naSkk3JYytIz9tQEul6S0F363/dG0lKGFQkqgA4VS6A+4pKUkjCk0IbMzOXYG2/GseTrDun88a5+9\nLR/Jsi1rOPq+Xy+xpb3WXntt63CyzqNnrZWbSOt3FloAmIgT6h7a+kC9LIkcP+mUTgAuOue0etmT\nunsAaGtfDsBQ/b+7UG3zSHVnySfynXnGmnrZjkc8at22vr1+bqjgy89VY3DXLI0cYx75TVIqQ/qX\nZXbv9oj0Y9v9r8qlzPJ1ySpvtTjxr5JZAq5a1GcjWZBuw1MrLgEenlR2MVD/c0sIYdDMtgGnm9mZ\nIYQHJ9W/LNNm4md4asXFDdq/kFl8XzxvQxe3agMDEZFFRaMjEVloro/H95tZd3LSzFqBDzeo/1nA\ngD+Jkd+k/mrgA5k6ic9n2u/K1C8B/+uEey8iIota00aORWRxCiH80Mw+BvwucLeZfZl0neNDHJlf\n/KfAS2L5HWb2TXyd41cDPcD/DiH8INP+zWb218CbgHvM7Cux/Zfi6Re7gBoiIrIkNe3guFbx9IFa\nJjXBcv5992pPKVzZs75edv+DnpowOhrr5tOgekeXX/fUs31t4g3rTqmXHTw0DMDArkEA1m1My0aH\nvSzZzW79xo31sn37PAUiN5FZTznugpcreUplLpemQIQ4sTDkPfViYnyoXrb9fk8BsbJPBuxe3ple\nF/84UMl7WsUoaZvk9N9/WbDega9D/DZ8F7tkB7v3EXewS4QQJszsF4B3A7+GD6qTHfLeGUL4+wbt\nvwVfau23gTdPav9xfI1lERFZgpp2cCwii1cIIQB/Gb8m621QfwxPiZhRWkQIoQb8WfyqM7MzgWXA\n1mPrsYiINIumHRyXY4A0s8kcEzGyai3+2LWW1nrZSM3PVeIyarVcWpYr+rkCI153PJ3J94Of3ALA\nnoNetnnz5nrZcL+vCJUr+0T5yy+/qF6294DvzrdiedpWvt2XdysVfOJeJd3PoJ4cno/fHdidRpwH\n9/rKVt0dfn1nW9r3pIlaMT5zJV1qLpdZDk5kKTGzdcDeOEhOzrXj21aDR5FFRGQJatrBsYjINN4J\nvNbMbsJzmNcBLwQ24ttQ/9P8dU1EROZT0w6OJ/Ac3exWVyNl/2k85iMPDIzWy6zqEdlSjBxXLf2n\nObDvEAD793kub++paVmpzaO1ZXwJuK9+49/rZT2rfSJ8T6cv1zYyXqqXBXyTknwuPUcMYhXiUm4t\nmb5X49J05bJHmnc/mP7Vd5l3mRVdvpxcIZ9uLFJs8/YHx8biPdJQej7XtL9+kaP5N+B84EVAN/5W\n8QDwF8B1Ma1DRESWII2ORGTJCSF8B/jOfPdDREQWHq1zLCIiIiISNW3keKTmKQpj1XS5snLwfIUf\n/PhWAHbtPFAvKyQT8iY8PYJamppQavMUiFLR0xayk/U29D4ZgDsf/ikAewfG0rIzvOyVr34FAOec\ne2bawarvcNe395H6qbY23+9g25C3UcynKRctcfLcQJ/3+eCux9Oy+FssFmJKSC2zBFx8/kLJ+1zM\nrN6W12cjERERkcNodCQiIiIiEjVt5Hj7ricAsPZl9XOVuJTbj396OwDDg+kyaq0xctwSJ7OVLW3L\ngp9r7/DNQ9asP61etn/Ml2Tbc9A3/Bgup9d1rPQl2Tad8RQAulb11Mue+rRnArD11sH6uZGiR6jv\nv+Uub/NQuuxaPm4CcvCAL9s2UU5DwK0tPilwZNwn7Y2NpRMNV6z0DUXGJ7xsoC/dPKRAERERERFJ\nKXIsIiIiIhI1beT4u9//MQBPv+ji+rlQ8zzfciU+tqWR02QvgPZWLyvn03+aiapHmA8N+XHlut56\nWeeAtzEy7m2XWtvrZXv2+kYdtfgZJJP+TLL/Rlua2sz2HQ8CMNbv0eH77nmsXpZLto+uxcXpCmnf\nS8s9V3lgaMCvH0/znnOtHn2uxTxky3weqpa1fbSIiIhIliLHIiIiIiKRBsciIiIiIlHTplXcc+e9\nAJz2tGfVz61e6RPqavW0ivTxO1d4mkJXhx/zy1bWy5a1+151IzELYWg8na132pk+2e7XXvd6Lxs+\nlF7X6qkMra3+GWRsfKBeNjZ0EIDxgXQ5ubv+84cAPP6AT+7rLqVLxlXj7n5DE54ykcukb1SCzwKs\nxV0Ba6S5GmPxuhCXsatltgyslquIiIiISEqRYxERERGRqGkjx8kSZhs29tbPdS/3aPCydj/2j+6t\nl7Uu8w03rBCjsLl0GbUkSluLk+AGx9NNNrraveylL3sZAOXRg2lZm0dmbcKXVjt4ML3fge1bAejb\nm066a8Xv2RH8uNzSCXMbz/Bl4PrHPPS7fX8ahTbz+sUu36SkLTPPbrTszzM+Hp8rU5bTZyOROjO7\nCbgkJH9mERGRJalpB8ciIvPt7p399F79jfnuxpKy4yNXzHcXRGSRU+hQRERERCRq2shx5zLfGW91\n94r6uWrcJa5Q9MfOZdYyDjn/voLXKRbSSW2VnOcijMa1kKu5dI3havx8MTjgO8/lq+nudFXzVIZc\nLe7EV0vXHy7kva18Pk3R2LTOJwzuOejpGMVimgORL/tOfB3Llscz6WS9frze3n6fDNi9/kn1soE9\nfl2It6lV0/uF9FuRRcXMLgDeA1wMrAYOAncBnwkhfCnWuRJ4KfAMYD1QjnU+GUL4QqatXmB75ufs\n/zNuDiFcevKeREREFpqmHRyLSHMys98CPglUga8DDwI9wLOAtwJfilU/CdwDfA/YDawCfhG4wczO\nDiF8INbrA64FrgROjd8ndpzERxERkQWoaQfH5bhUWv8TD9fPrVrtk/TWbuwE4ND+ffWywWGfkDdc\n8Lk4HS2ZOTlxUtuhskeA79u+rV509pkdXmXYo8IdmYl8nXmPPnfGCX0tbeV6WWGNT7ArH+pJ6x/o\nB+D0Hr/38rVp1Hv/E152KEaHO6vpr64WI9OHJrz98mBLep/gkfBq1eIxXcst2RVQZLEws3OBTwAD\nwPNDCPdMKt+Y+fG8EMK2SeUl4FvA1Wb2qRDCzhBCH3CNmV0KnBpCuOYY+3TrFEXnHEs7IiKyMCjn\nWEQWk7fgH+o/NHlgDBBCeDzz/bYG5RPAx2MbLzyJ/RQRkUWqaSPHE32eA/xv//y1+rnuNR45Hj/o\nUdjWYpo7PBaXOsvFRNz8RBphbal6VHn/wUEAbrr55npZPue5zadv3ARAKfMvWiomect+rNTSzyJW\n8Ohu2/Ll9XNty9oAePbTz/L7HdqfNtbmfciNev+KmazI2rBvGrK205+np7e7Xlbc7X1+dKdHnI3M\nLiBoExBZdC6Mx28draKZbQL+AB8EbwLaJlXZMBsdCiFsmeL+twLPnI17iIjI3GnawbGINKUk12jn\ndJXM7HTgJ8BK4PvAjUA//omwF3gD0DLV9SIisnRpcCwii0lfPG4A7pum3rvxCXhXhRCuzxaY2Wvx\nwbGIiMgRmnZw3Jr3v6D27TpUP7fzIQ82FWOqteXSx6/WPN0gxIlrAwMT9bJynMS2YkUXAHt2p6mM\nK1fcBsB5Z/ncm85SOsmtUPC0hUolpkK0pCkUuXafFJhrSZdk6163FoCeZV42vD9Nq2itelsrS56i\nUczMFyzkPOWiPOF97synkwJXd3iqxeMVPxdqmXyMnDYCk0XnFnxVipcw/eD4yfH4lQZll0xxTRXA\nzPIhhFnJOTpvQxe3alMKEZFFRRPyRGQx+SRQAT4QV644TGa1ih3xeOmk8suB35yi7QPxuOmEeyki\nIotW00aODw74JDWzzCS4ONluIkaJy4XMJhut/k+RL/hxdCKNvoZkY5CiH4eH0rLHH/NodCFuGpLL\nZ6KxITn4uZDZW6DQ5su7Fds66ufaY2R6dCyJWqcbkRRKHh0mLidXyASAO4telsSlH78vXb4u1+5j\nhTXL1gCwt5xuRBKqafsii0EI4V4zeyvwKeBnZvY1fJ3jVcCz8SXeLsOXe7sK+Ccz+zKwCzgPeDG+\nDvJrGjT/HeDVwD+b2TeBUeCREMINJ/epRERkIWnawbGINKcQwqfN7G7g9/DI8MuB/cCdwGdinTvN\n7DLgj4Ar8Pe6O4BX4nnLjQbHn8E3AflV4PfjNTcDGhyLiCwhTTs4Hst5xLiQy0SOk0BxjOBaJnF3\nZYzaTlS9UsilUdVq3HK5GvfwCLX0unLFT+YLfp98NhgbI9Wh5m2OjWW2lh7zyPbweBqF7usfAGA8\nbkU9kE8n0z824vVGxz0Vcngk3VAkF+sVcyv9ucbS5doqFc857m71zUZKPWkHh8oDiCxGIYQfA79y\nlDo/An5uiuIjEu5jnvH74peIiCxRyjkWEREREYk0OBYRERERiZo2raIcd3/LZ9Iqkg3qkqSD5cvS\nyXDJDnkhTuDr6Oislw2PeDpEuRxTLkJ2kp//dbZWi0utrVxVL+uIfRgd9vSFkdE03WH/wX1+3Lev\nfm50eCS277+WPdU0rWJn2SfdVczLznjK2fWyzjZPp3h8u++G1xl3zAOo1HwHv4EJT6fIZn30rFqL\niIiIiKQUORYRERERiZo2ctwSJ8MVaula/mU88ptv8Ulq5WpaNhGXZyu1++YhXSvb6mX5OMuuP06U\ny+WK9bL6Mm1x0l126biceZkFX35tsG93er8xj/J2LU8j1KW42cgjjx0E4OG96UYkG899HgCnnf4U\nADpKK+tlO+571Ps3sh2ASjWdaxRqHn0uxNmI1YlB0sL0GUVEREREkWMRERERkToNjkVEREREoqZN\nq+iM6Q5WS7eSy+f8XDluXTc+ku4W11LyyXkWrxscGqqXLe/yvefG4prEY6PpznoHDviOs3fedQcA\np6y4uF7W1tIKwMH9nk6xf8+j6f1ynjIRSNtqjfVP2XAaAOvPe0q9rGPlegAGhrzvt9z0s3rZroe8\n/ZFBT8uopksgkw9+LsT7FYvpusqH7eYnIiIiIooci4iIiIgkmjZyXCl4VDSXecL2do8OD8SIcYF0\nYl1bmy95ViHZgS6NsLa1+7n2Dq8zOn6wXjZ48BAAX/zbvwNgVTFdfu3Fz97i99v1gPdlIr2Okn8u\nKbWtqZ+qxl32etb4bnbF9tPrZQcO+XJyj93rk+4OPbY/vW44TtxLguSZ9dpq9TCyP4OFNJJeCNmF\n3UREREREkWMRERERkahpI8dta7sBKLal4/+Wki9dduihnQDkM58NymWPvtZKHk21QhpVPnDAN/Ho\n6PDrqyHNEy7FnOaBQx4VHuh/ol722EO3+f12e+S4uyfddKSl3SPGA6Np9Lbf9wBh7KD3pRTSpd8e\nf9TbffDO+wEIg2licSkuSVfLeV+SZeX8QeISc/FZcyGNbFs1fUYRERERUeRYRERERKROg2MRERER\nkahp0ypyOR/3J5PwAELcOa5Q8MfO1dKlzCo1T1MIccJasTVNP5gY9Ql8ExVfFq2YT9vM4TvOnXnm\nBgB6n7y8XvbVr30agNVx173udc+rlw0Neh8e3rYn7UPVl3K75+5tfr+RdPLcaNzBj4o/Vyinn2vy\n8ddYsLjjXy5NqwjJM8adAkuWpnEU0FJusvCY2Q6AEELv/PZERESWIkWORURERESipo0cd3WtAKBY\nSiedjY74cmi5uBmIZQKnHcs8ajsSN83I5dOobUubt1Ete/Q1n0ujysk+Guc95SwABocO1MuGJvoB\nuOD8iwCYmFhZL9u2bRcA3ctPq5+7+46HADj4+LC3nfnsUir4PWshTqzLpxFgi0uyVWpjsU/pr9WS\n56jGaHLmurw+G4mcVHfv7Kf36m+ctPZ3fOSKk9a2iMhSpdGRiIiIiEjUtJHj0RFfF228WqqfG4vn\nCgWPnrYU07L2zrhM25hHbTtXLquXTcRNQ4aHPPJcq07Uy7pXeI7xuWc9FYBbfvjdelln25MB2Lff\nI88/+EG65XNnaxcAfbl99XNP7PBIc6nqOc2lUvrZxYKHqKv17bAzOcdxp5Mw7pHtYimNDsfUaybi\nZSGTZ5wz5RzL/DAzA94GvAU4AzgAfBV4/zTXvBZ4E/AMoBXYDnwR+JMQwniD+ucAVwMvBNYCh4Dv\nANeGEO6fVPd64A2xL1cAvwWcCfxnCOHS439SERFZbJp2cCwiC9p1wNuB3cBfA2Xgl4HnACVgIlvZ\nzD4LXAU8DnwF6AMuBD4EvNDMfiGEmBPl9V8M/DNQBP4VeAjYCLwSuMLMLgsh3NagX38OPB/4BvBN\nkq0lRURkydDgWETmlJldhA+MtwEXhBAOxvPvB74LrAceydS/Eh8YfxX49RDCaKbsGuCDeBT6z+O5\nlcDfAyPAC0II92bqnwfcAnwGeGaD7j0TeEYIYfsxPM+tUxSdM9M2RERk4WjiwXFMGQhp+kGycVxP\nj+9OV45LtAGMDXtKw+rVvrNee2d7vWxX3yEA8sRd6fLp7nQXPPv5ALTlPU3igXuH6mU9RT+3/a57\n/P6VdIm1jpL/FbgQ+urnKmNxGTl88p1V64EwiiU/F6p+73IlLcsnS9Pl4zJvIZ1MaPHfoViMkwpr\n6XWmtAqZH1fF4x8nA2OAEMKYmb0XHyBnvQOoAG/MDoyjDwG/A/w6cXAMvB5YAfxOdmAc73G3mX0a\neKeZnTu5HPjfxzIwFhGR5tPEg2MRWaCSiO3NDcp+QCaVwczagfOB/fiAtlF748DmzM/PjcfzY2R5\nsrPicTMweXD8k+k63kgIYUuj8zGi3Cg6LSIiC1jTDo5rMXo6PDhcP1ce92htz0qP6JZCJpKb86Xc\n1qxZDYCV0n+awroeALbv2AFAZ/eKetmLfuHnAXggLsPWdyBt0+Kyaa1lP3a0pW0mG5KETB/y+WTS\nnUd3LTvpLrZVbPHnqlqaCpkr+XW5WlzmLfPv0NLiz1Ueifex9H7JZigic6wrHvdMLgghVMxsf+bU\nSvzPQGvw9ImZWBWPv3WUessanHtihvcQEZEmpaXcRGSu9cfj2skFZlYAVjeo+7MQgk331eCa849y\nzd826FtocE5ERJYQhQ5FZK7dhqcbXAI8PKnsYqC+FmEIYcjM7gGeYmbd2RzladwC/Aq+6sSds9Pl\n43Pehi5u1UYdIiKLStMOjidiCsVEZkGo8TE/NzDggaWN3d31slJMWxju8x3uxsrphW0xNaFrma8/\n/JyLnl8vW7PKg1xf/MkXAKhW0+VWawVPYSgku9Jl2iwUfI1ly6wUVY3fF1o8oG/ZGFbOf8gXva1q\nZnLfeM3brYS4g59lJiHGRkLcFbBcSe9nmUl9InPoeuA3gfeb2dcyq1W0Ah9uUP+jwN8AnzWzK0PI\nzGKlvjrFaZml2T6Hr5f8QTP7aQjhJ5Pq5/BVLG6axWcSEZEm0bSDYxFZmEIIPzSzjwG/C9xtZl8m\nXef4EL72cbb+Z81sC/BWYJuZfRt4FOgGTgNegA+I3xzrHzCzV+FLv91iZt8B7sFTJp6ET9hbhW8k\ncjL1bt26lS1bGs7XExGRaWzduhWgdz7ubdllv0RE5kJmh7y3AaeT7pD3PuAOgBBC76RrfgkfAF+A\nL9V2EB8k3wh8IYRw36T6vcDvAZfjg+IJYBfwU+ArIYR/ydS9Ht8h77QQwo5ZesZxPEXkjtloT+Qk\nSNbivm/aWiLz43ygGkJomesba3AsInISJJuDTLXUm8h802tUFrL5fH1qtQoRERERkUiDYxERERGR\nSINjEREREZFIg2MRERERkUiDYxERERGRSKtViIiIiIhEihyLiIiIiEQaHIuIiIiIRBoci4iIiIhE\nGhyLiIiIiEQaHIuIiIiIRBoci4iIiIhEGhyLiIiIiEQaHIuIiIiIRBoci4jMgJltNLPPmtkuMxs3\nsx1mdp2ZrZyPdkQmm43XVrwmTPH1xMnsvzQ3M3uVmX3MzL5vZgPxNfWF42zrpL6Paoc8EZGjMLMz\ngB8BPcDXgPuAC4DLgPuB54UQDsxVOyKTzeJrdAewAriuQfFQCOFPZ6vPsrSY2e3A+cAQ8DhwDvDF\nEMLrjrGdk/4+WjiRi0VElohP4G/Ebw8hfCw5aWYfBd4F/DHw5jlsR2Sy2Xxt9YUQrpn1HspS9y58\nUPwQcAnw3eNs56S/jypyLCIyjRileAjYAZwRQqhlyjqB3YABPSGE4ZPdjshks/naipFjQgi9J6m7\nIpjZpfjg+Jgix3P1PqqcYxGR6V0Wjzdm34gBQgiDwA+BduDCOWpHZLLZfm21mNnrzOx9ZvYOM7vM\nzPKz2F+R4zUn76MaHIuITO/seHxgivIH4/GsOWpHZLLZfm2tA27A/zx9HfAfwINmdslx91BkdszJ\n+6gGxyIi0+uKx/4pypPzK+aoHZHJZvO19TnghfgAuQN4KvBXQC/wLTM7//i7KXLC5uR9VBPyRERE\nBIAQwrWTTt0NvNnMhoD3ANcAr5jrfonMJUWORUSml0QiuqYoT873zVE7IpPNxWvrU/H4ghNoQ+RE\nzcn7qAbHIiLTuz8ep8phOzMep8qBm+12RCabi9fWvnjsOIE2RE7UnLyPanAsIjK9ZC3OF5nZYe+Z\ncemg5wEjwC1z1I7IZHPx2kpm/z98Am2InKg5eR/V4FhEZBohhG3AjfiEpLdNKr4Wj6TdkKypaWZF\nMzsnrsd53O2IzNRsvUbNbLOZHREZNrNe4C/jj8e13a/IsZjv91FtAiIichQNtivdCjwHX3PzAeCi\nZLvSOJDYDjwyeSOFY2lH5FjMxmvUzK7BJ919D3gEGATOAK4AWoFvAq8IIUzMwSNJkzGzlwMvjz+u\nAy7H/xLx/Xhufwjh92LdXubxfVSDYxGRGTCzJwF/CLwYWIXvxPRV4NoQwqFMvV6meFM/lnZEjtWJ\nvkbjOsZvBp5BupRbH3A7vu7xDUGDBjlO8cPXB6epUn89zvf7qAbHIiIiIiKRco5FRERERCINjkVE\nREREIg2ORUREREQiDY5PkJldaWbBzG46jmt747VK/BYRERFZADQ4FhERERGJCvPdgSWuTLoVooiI\niIjMMw2O51EIYSdwznz3Q0RERESc0ipERERERCINjhsws5KZvcPMfmRmfWZWNrM9ZnaHmX3czJ47\nzbUvNbPvxuuGzOwWM3vtFHWnnJBnZtfHsmvMrNXMrjWz+8xs1Mz2mtnfm9lZs/ncIiIiIkudIPBi\nCwAAIABJREFU0iomMbMCcCNwSTwVgH58e8Ie4Gnx+x83uPYD+HaGNXxP+g58v++/M7O1IYTrjqNL\nLcB3gQuBCWAMWAP8KvAyM3tJCOF7x9GuiIiIiEyiyPGRfg0fGI8AvwG0hxBW4oPUU4HfAe5ocN3T\n8T3DPwCsCiGswPem/3Is/7CZdR9Hf96CD8hfDywLIXTh+97fBrQDXzKzlcfRroiIiIhMosHxkS6M\nx8+HEL4QQhgDCCFUQwiPhhA+HkL4cIPruoAPhhD+KITQF6/Zgw9q9wGtwC8dR3+6gDeFEG4IIZRj\nu7cDlwMHgLXA246jXRERERGZRIPjIw3E4/pjvG4MOCJtIoQwCnw7/njecfTnEeDvGrS7H/ir+OOr\njqNdEREREZlEg+MjfSsef9nMvm5mrzSzVTO47t4QwvAUZTvj8XjSH24OIUy1g97N8XiemZWOo20R\nERERydDgeJIQws3A/wQqwEuBrwD7zWyrmf2pmZ05xaWD0zQ7Fo/F4+jSzhmU5Tm+gbeIiIiIZGhw\n3EAI4UPAWcB78ZSIAXyzjvcA95rZ6+exeyIiIiJykmhwPIUQwvYQwkdCCC8GuoHLgO/hy999wsx6\n5qgrp8ygrAocmoO+iIiIiDQ1DY5nIK5UcRO+2kQZX7/4WXN0+0tmUHZ3CGFiLjojIiIi0sw0OJ7k\nKBPbJvAoLfi6x3Oht9EOe3HN5DfFH/9pjvoiIiIi0tQ0OD7S583sc2Z2uZl1JifNrBf4W3y94lHg\n+3PUn37g02b263H3PszsaXgu9BpgL/CJOeqLiIiISFPT9tFHagVeA1wJBDPrB0r4bnTgkePfjusM\nz4VP4vnOXwD+xszGgeWxbAR4dQhB+cYiIiIis0CR4yNdDfw+8H+Ah/GBcR7YBnwOeGYI4YY57M84\ncCnwh/iGICV8x71/iH353hz2RURERKSp2dT7S8h8MrPrgTcA14YQrpnf3oiIiIgsDYoci4iIiIhE\nGhyLiIiIiEQaHIuIiIiIRBoci4iIiIhEmpAnIiIiIhIpciwiIiIiEmlwLCIiIiISaXAsIiIiIhJp\ncCwiIiIiEhXmuwMiIs3IzLYDy4Ed89wVEZHFqBcYCCGcNtc3btrB8RnP/Z0AUKtVMmd9ZY4Qatkf\nDxOSk7W00OL3gdoR9S05Nlr0ww6vf6wrg1j2h2O41Bqds0Zn/Q8H226/vlGhiJyY5W1tbd2bN2/u\nnu+OiIgsNlu3bmV0dHRe7t20g2MRaS5mdhNwSQhhxh/mzCwAN4cQLj1Z/ZrGjs2bN3ffeuut83Br\nEZHFbcuWLdx222075uPeTTs4rlk+HjMh1/h9qHnENGQyrpP/2taSEG3mulzyfTiyLI3o2uQTTA73\nhuzPxxpFnkH1ZMiQjVdb0q8GwwlTyrmIiIjIYZp2cCwiAmwGRubr5nfv7Kf36m/M1+1FRObVjo9c\nMd9dOC4aHItI0woh3DfffRARkcWlaf+uXsOoYQTL179qyVfOv6qWq39V4lfV8v6VL9S/ygX/qhaK\nVAtFalZIv3LJV2wzl8t8FajmCtRyRf+yUv2rmitSTc7HrzDpK227QDWX96/81F9JHw7/ylHL5Qi5\ngn/l069a/BKZb2b2MjP7jpntNrNxM9tlZjeb2Vsb1C2Y2fvM7MFY9zEz+3/NrNSgboi5ytlz18Tz\nl5rZG8zsZ2Y2amZ7zeyzZrbuJD6qiIgscE07OBaRxcHM3gR8DTgX+Ffg/wO+CbQBVzW45O+A3wW+\nD3wSGAV+H/irY7z1u4BPAXcA1wH3x/v9yMzWHPODiIhIU2jasGE1FyfdZefAxUlpyZJqtUzhEfPd\nMhPYkjKr+lS3Qj79TFHIeWmlmtRNL8zFzx6tRf9nrmSWhxuv+BJz2RXWcuHwSX3hWNZvO0xmGbrY\nH8sdOSMv6LORLAy/DUwA54cQ9mYLzGx1g/pnAE8JIRyMdd6PD3Bfb2bvDSE8McP7vgR4TgjhZ5n7\n/RnwTuAjwH+fSSNmNtVyFOfMsB8iIrKAaHQkIgtBBShPPhlC2N+g7h8kA+NYZxj4Iv5+9qxjuOcN\n2YFxdA3QD/yambUcQ1siItIkmjdybD7uzy5rNjly3GhnjXpJo6XWYpv1I9DR7o2Wxz10XK5mospx\nrbjOrlYABkcn6mWDI96zfCaim7ck2h0OO8abHtmfY9Iocqy9P2RB+CKeSnGvmf0DcDPwwxDCvinq\n/1eDc4/F48pjuO/Nk0+EEPrN7HbgEnyli9uP1kgIYUuj8zGi/Mxj6I+IiCwAihyLyLwKIXwUeAPw\nCPB24KvAHjP7rpkdEQkOIfQ1aCbZCjN/DLfeM8X5JC2j6xjaEhGRJqHBsYjMuxDC50MIFwKrgCuA\nvwFeAHz7JE6OWzvF+WS1iv6TdF8REVnAmjatohKXKAu1av1cPWUil6Qt1CZfhtXiucxMuZCkO9Qn\nyqVaWn31qOGap0xUs5P8at5GDe9DqZQGtarjfp9aJrOh1FL0c3HiXyYLg1zsz/ST9KZOk2i84a4+\nG8nCEqPC3wS+aWY54I34IPkrJ+F2lwCfz54wsy7g6cAYsPVEb3Dehi5uXaSL4IuILFUaHYnIvDKz\ny8ys0ce3nng8WTvc/YaZPWPSuWvwdIq/DyGMn6T7iojIAtb8keNsBLiWRH5rsSz72cDLCskUvlxa\nVouT1kOcTF/LfKYoFnLx6FHf4cx8+0q8dVurly3LZEPujBPyapngdSm2taLL7zeWaWvvoFdMloyr\n1VMs02dMlm3LhoknR4wPn+KnCXmyIHwVGDKzW4Ad+J9Ang88G7gV+PeTdN9vAT80sy8Bu4GL49cO\n4OqTdE8REVngFDkWkfl2NfBTfGWHt+IbcRSBPwAuCyEcscTbLPmzeL+n42sbnwNcD1w0eb1lERFZ\nOpo2clxOoqmZCHAuLpuWa/AX3Fo1yU32CG2plF43jucVh6pfV8gsv7b5NN+j4M5HBwHYO54mCufM\n/5t+zqblAIxlItV37fHIb0tr+is4dU0HAGet9XPVUme97Os/8/9Wl0f9usMi4vXvJx9TjTKVQ+NE\nZJE5FUL4FL5T3dHqXTpN2fX4wHby+Wlf5FNdJyIiS5cixyIiIiIikQbHIiIiIiJR06ZVECfk5TPp\nB/m8fxaoVDw1wTKfDfI5nzTX2er1V3Wks+d2DXu9cbzO2etb62W/ePFmAB79xp0A1A6m6ZHr2z1V\nY8tZni5x//70OisMALCssy2t3+N7Djw+4JPzH96XLrNai5MJK3HSXrC0f+lfji3zv7EsOTZIJTF9\nNhIRERE5jEZHIrKkhBCuCSFYCOGm+e6LiIgsPE0bOc4VfDm07F6yrUX/LDARJ8aNpauh1SfZdbT7\nFas603+a/jhXr6M0BsALt6yvlz1t80YAln13mx9b0iVZz/S5emzu9Q2+nhhPp8UVij7Jb6yaRnS3\nPuFR51M6vQ/dbekGJsWSR58H+5Pl5GY6mS5Gk+uTAdPrzPTZSERERCRLoyMRERERkahpI8fEHOJq\nSKOvpXiurdXzfMuHLbsWc3pj/fHM5hwr2j3E/NRNqwD4+YueWi9bvdojusuXeSR4TT7NEz5rnUev\nT32SR5pXPtFXL0six8ta0l/Bmeu8X2f1jALQ0/2ketmdT3j/tv7wsfh4LWkHY0C60aJVycZjoWGU\nWJ+NRERERLI0OhIRERERiTQ4FhERERGJmjetIu+T2kJm/F+Mk/TGg+dMVDKpBi0Fr18reFk5pBPr\nnnv2CgBedumzANi4cV16m6KnLfR2xdyG9elSbs/cfC4AXd0rAVi1fKBe1lryFI+WUpoe8dLnnwHA\nGeu9Dy0t3fWy8l2ektH5X3sAGLXSEY9ci49TO6IEUFqFiIiIyFFpdCQiIiIiEjVt5Hh5jMhO1NLl\n0/JxEtzohE94KxbT6GtH0SPHubxPyDt1bVe97JQV/n2x1SffFQrpP1shXnfqKb5cW1dpc71s0/oY\nYQ4eTX7S2s562elrlgPw8KE0zjtcjZuUjPpEwf+8a0e9rFyLk/XWLgNgRzrvj/Hg11Vz/qwhfeS6\nXM77mc+li9s1jDCLiIiILGGKHIuIiIiIRE0bOV7Z0Q7A4NhY/VyyffTyFt/GuUYmxBqjr7mYe3zG\nxva0LObrHuj3iPNZmchxa4zWrlzj+cEj1TQyO5FsuFH1SPCarrTNp53hy7sN3H+gfu6xR3YDUNm9\n068fTT+7bHnuxQDc/8QgAB0d6TJ0Dx+Km5pUPeqdXdItFz//FGLE2DKbgFQahZhFREREljBFjkXk\nMGZ2k5md9E9OZtZrZsHMrj/Z9xIREZkpDY5FRERERKKmTavo7vAJbB2lTJpDxYNhrTmfiDdWSVMT\nBstetrbd/0lO72mtlz0xED9DxCXgBkfG62VrlvnEv7YWb3N1z9p62UjZl1+rTPgOe4XMZLgn9fjk\nvvMn0jSHU9b6vTe1e3rEg7vT5eQqFW9jU7f3pbOlWC+z2O72QyH+fORWeWkYMC0r0GBLPRF4PdB+\n1FoiIiJNqGkHxyJyfEIIj853H0REROZL0w6Ol7d5RLdlWRoB7otLpI3GCHLIbIyRbP7R0+lR4fUr\n0sjsrn6PsHbENkdGR9Mb5bz9Qs7/KVcub0uLKh4BPjQQl47rTMuKcQm455y7oX6uMjEMwHDNl46b\nqKSR49HhIQAuPNeXh9u1Z7he1tnmS8UdGPVnqGYCwrUYHa7m/FmzseKcsmqWDDO7Engp8AxgPVAG\n7gI+GUL4wqS6NwGXhJBO7TSzS4HvAtcC3wQ+CDwXWAmcFkLYYWY7YvXzgT8GXgGsAh4GPgV8LISj\nzwI1s7OANwI/D5wKLAeeAL4N/GEI4fFJ9bN9+5d47+cBJeCnwHtDCD9qcJ8C8CY8Un4u/n54P/A3\nwCdCCFrtUERkCWrawbGIHOaTwD3A94Dd+KD1F4EbzOzsEMIHZtjOc4H3Aj8APgusBiYy5SXg34EV\nwD/En38F+HPgbOBtM7jHK4E34wPeH8X2nwL8JvBSM3tWCGFng+ueBfw+8GPgM8CmeO/vmNnTQwj3\nJxXNrAj8K3A5PiD+O2AMuAz4GPAc4Ddm0FfM7NYpis6ZyfUiIrKwNO3guCXmGncU00fMxy2l98cI\ncltrGh1eFvdeXtexD4AVy9NtnZP/9re0eFstuTTiOjbmZXE3aIYm0mBT7yaP8vY/sQOAU9LdoEmC\nUqdtWFk/Nz7sy84Vq37vs/JppPkp56z2Z8jFjUiKaR96ez16vXNsLwBbd2ci4nkP/jUK11nDLaWl\nSZ0XQtiWPWFmJeBbwNVm9qkpBpyTvQh4cwjhr6YoX49His8LIYzH+3wQj+C+1cz+MYTwvaPc4wbg\nz5LrM/19Uezv/wDe0uC6K4CrQgjXZ675bTxq/Q7grZm678cHxn8JvDOEUI3188BfA280sy+HEL52\nlL6KiEiT0ehIZAmYPDCO5yaAj+Mfkl84w6Zun2ZgnHhvdmAbQjgIfCj+eNUM+rpz8sA4nr8Rj35f\nPsWlP8wOjKPPAhXgguSE+afC38VTNd6VDIzjParAe/DPk79+tL7Ga7Y0+gLum8n1IiKysDRt5FhE\nUma2CfgDfBC8CWibVGXDERc19pOjlFfwVIjJborHZxztBmZm+MD0Sjx/eSWQz1SZaHAZwH9NPhFC\nKJvZnthG4iygG3gQ+B9+uyOMApsbFYiISHNr2sHx2i7Pc6iU03OFvD/uREwysMxWcrl47tSV/t/g\n7tUr6mVdywYAKMYd9nq6O+plw8M+aW5ZXAJucKxSL9vY423sGPAJdrVqmtzQFtMiQjWt/7TNPj7Z\nt8eXhfvp9l31son/uheADRt6AHhg+8F62fnnngvAFVs8jWPkR2nZ3jh3sJ5CkXnmkNMfDpYCMzsd\nH9SuBL4P3Aj0A1WgF3gD0DLV9ZM8cZTy/dlIbIPrumZwj48C78Rzo78N7MQHq+AD5lOnuK5vivMV\nDh9cr4rHM/GJhVNZNoO+iohIk2nawbGI1L0bHxBeNTntwMxeiw+OZ+poq02sNrN8gwHyunjsn+5i\nM+sB3g7cDVwUQhhs0N8TlfThqyGEV85CeyIi0kSadnC8aaVHjkdG0wlyYzWPmuYL/t/3ajV9/ELw\noNOp6z0qvGLlqnpZV6tHjolR3q7OdCLfoT4PSBVyfp9QTUPVLUW/z5oe/4vu+NCBelmp4G3VMkON\nlhb/S/fOuEzbYwfSqPJ42SfrtcRJhf19A/WyO+7aCsBFz3omAM89M/2r8833eVtl82fN/gk5aA+Q\npeLJ8fiVBmWXzPK9CsBFeIQ669J4/NlRrj8dnwtxY4OB8cZYfqLuw6PMF5pZMYRQPtoFIiKydOjv\n6iLNb0c8Xpo9aWaX48ujzbYPm1k9TcPMuvEVJgA+d5Rrd8TjxXHliKSNZcCnmYUP9CGECr5c23rg\nL8xscv41ZrbezM490XuJiMji07SRYxGp+wS+SsQ/mdmXgV3AecCLgS8Br5nFe+3G85fvNrOvA0Xg\nVfhA9BNHW8YthPCEmf0D8KvA7WZ2I56n/Av4OsS3A0+fhX5+CJ/s92Z87eT/wHObe/Bc5Ofhy73d\nOwv3EhGRRaRpB8fLYuZDp6VpFbXgaQrdJc8nGMisSdwZUyCeetaTAFi1Il2UuL09ToyLE/ryhXTu\n0srl3sbuvP9ltpiZ9lOLaZc9Ma1iXzXdWa+txdMdisU0t2H5Mm930ya/972PrK6Xlcv+F+ZcbL+Y\n+UtwsudYMr/uuZvTvh+MayffvzuujJVLU0Isd9TNyqQJhBDuNLPLgD/C1wIuAHfgm230MbuD4wl8\nZ7v/hQ9wV+PrHn8Ej9bOxH+P17wG3zRkH/B14H/SODXkmMVVLF4OvA6f5PdL+AS8fcB24APAF2fj\nXiIisrg07eBYRFJx++Sfm6LYJtW9tMH1N02uN829+vFB7bS74YUQdjRqM4Qwgkdt39/gsmPuWwih\nd4rzAd9w5Ibp+ikiIktL0w6OKxMeKc0fNmneo7zLWuLueS0j9ZINPb582vIOTz/ctz9dsapaGQKg\nVPXrDj2WTnhr6fCVqQoxfNuSTyO648Peh1D0iHUul4aVW80jusssnXNUHvOd7npW+aTATWvTVMih\nA97uKRt8ebiB0XSy3oEx/zU+uNP72VpKU8lPX+OR4v5DPoFvaCLdW0E75ImIiIgcTqMjEREREZGo\naSPHo8MeKe1sTaO1VvFo68jBQwC0FNLo6yN9/n3fAc8LDrWxelnfQc8PHr/fo7w/3p9ustG23jfu\nWBnziiuZXUd+cmA3AKUWj94WCulffsf3+7Jue4bStqrDawHY1+cR4KHYT4COtnYADhzwfuUyv7rh\nIe/X/dseTp40feYY0V5u/nyZfUioVfTZSERERCSraQfHIjK3psrtFRERWUwUOhQRERERiZo2cpws\n09Y3kKYtjA96msLBQ35u2fLl9bLRsqci9Mb0g4ufeUa9rGOzf4bY9uhOAPaOt9bLevI+ae7Jq3sA\nKBXTvIX+fk/HaI/3qWbmBnZ1+z99rdheP7dv3Cf3/fShPQDseCydFJiLS7dVJnwyYHZyX1urt5Us\nBZctCxX/d4ib+zFRSVMuJqqTd/gVERERWdoUORYRERERiZo2crx7714ARob76udGR3ziWv9onDQ3\nni7l1tLi0d3RvR6ZfQor6mUbNq0B4BkbfIOQp25JJ+u1x008OjpiNNnSyPHQoC+b1trqEd2Jclo2\nOObfj9fSDUXu3OqT9B475GUP7cksCzfiEwzHRvx5Okrps27o9ujzxKD/Ogv5NHKci/2JwWVymRl5\ntXFFjkVERESyFDkWEREREYmaNnL84L13ARAyw3+Libdj4x75HRpNN+AoxM1CRns2AvCNsd31sv9a\n7VHkNd3LAFi3prNe1pL3nN6xMV8CbqKWbkndVvJ6SZbvxEQaqX5sn9+7byDdUKSv36PDh3Z5znFt\nbybnuOb960iShzNR376YAz0UI8ZpDyBU/adczC/ObiNW1WcjERERkcNodCQiIiIiEmlwLCIiIiIS\nNW1axb49PiGvVk0nteWCpyTUYupDsnscwARer7DCJ8VVBtJ/mjsfuw+AVat9F7y1q9LJeh2tvvvd\n/gM+UW7PwTRVoyfW72jxdIfK+HC97IFH9/t9Qzqzbt1qX8qtq9VTLSZax+tlhYLfJ5/3yXf7+0fr\nZcPj3vdaLk7ym0hTLibGvCyM+7OHWjbpQkRERESyFDkWEREREYmaNnJMJUZMa+mEt0qcdEcMGBvp\nkmcWJ7MlK52NjKbLtY3HtioVj76WK2k0upLMjyt7lHd4NI3o9g3GCXJl/wySD2lEN8SodblcqZ8b\ni9HdJKKdy6e/nmKpFK+zw64HiKfImd8nlymz4H0I+aRSWkYt873IEmdmNwGXhOT/ZCIisiQpciwi\nIiIiEjVt5NiIEdlwZI5tEi+1XBogyhc9p7cWo67VzNbKFhdAq1T8XLWSltVCIdaPS6bl0s8btdhG\n0pRZ2pexMY9Mj1XSX0ESkW4t2BFtlWL/8kWPIBcH03xkK3h02OJW1BXSaDkxcl6pB82zUWXlH4uI\niIhkKXIsIouOmV1gZv9oZjvNbNzMdpvZjWb23zJ1rjSzr5jZw2Y2amYDZvZDM3vdpLZ6zSwAl8Sf\nQ+brprl9MhERmW9NGzkWkeZkZr8FfBKoAl8HHgR6gGcBbwW+FKt+ErgH+B6wG1gF/CJwg5mdHUL4\nQKzXB1wLXAmcGr9P7DiJjyIiIgtQ0w6OQ7JPXGbOWS7nqQkUPGBeqx2ZOjExESfdZSarVeOsu6Eh\n3+Guo6VYL2srFWKTntrQ3pIuzdYaUyFaS8XYdpruUI0T5bA0taNY8LZaWr1/1Wrah6EhT8Noa88d\n1ieAkXGfBJhr9bZqhbR/uXZPtcjF6pVMKoWm48liY2bnAp8ABoDnhxDumVS+MfPjeSGEbZPKS8C3\ngKvN7FMhhJ0hhD7gGjO7FDg1hHDNMfbp1imKzjmWdkREZGFQWoWILCZvwT/Uf2jywBgghPB45vtt\nDcongI/HNl54EvspIiKLVNNGjqvjQwDkcukjhrhsmtX8M4FlYqe1GDnOm0d+a5kJeaHq140NeZsD\nhfS68RHf9GMwRpWHxtKJcpXly/w46n0YG0uXgEsm/CXRYi+Pk/RGvI3RkXRZuORzzKFD/V5nIrOc\nXOxOPh5rtcxKVPExCtUYVc6lUWVy+mwki86F8fito1U0s03AH+CD4E1A26QqG2ajQyGELVPc/1bg\nmbNxDxERmTtNOzgWkaaUbE+5c7pKZnY68BNgJfB94EagH/+42Au8AWg5ab0UEZFFq2kHx1ZNlnJL\nz4Uki2TCT1rusEI/lGN+8Gi6QcjEiG/7XC3HaG0tjQ63tnqkeXzM84mrmf0Dxse9XkuyNFs2oBs3\nDcnbkUu/jcb7VappXnE+RnyTFOXKRLpJSXLPJCJeGU37NzHuEe1g8VdtaeTYChobyKLTF48bgPum\nqfdufALeVSGE67MFZvZafHAsIiJyBP1dXUQWk1vi8SVHqffkePxKg7JLprimCmBm+SnKRURkCdDg\nWEQWk08CFeADceWKw2RWq9gRj5dOKr8c+M0p2j4Qj5tOuJciIrJoNW1aRa3mS5bls7kMMXeiUvX0\niFwlk1YR/yXKMd1heChNaRiNKRPJjnKlcmZSW0yrKMel1SqZtIqBgQEAqqOeJmH59LrxUZ9sVxtN\n0yNKMc2jVvP+VTNpFUk+RVwxjlolXRYuF1MmQrIj30SaVhHqj+jf5KqZnfW0mJssMiGEe83srcCn\ngJ+Z2dfwdY5XAc/Gl3i7DF/u7Srgn8zsy8Au4Dzgxfg6yK9p0Px3gFcD/2xm3wRGgUdCCDec3KcS\nEZGFpGkHxyLSnEIInzazu4HfwyPDLwf2A3cCn4l17jSzy4A/Aq7A3+vuAF6J5y03Ghx/Bt8E5FeB\n34/X3Awc7+C4d+vWrWzZ0nAxCxERmcbWrVvBJ1DPOQtB0UMRkdlmZuNAHh+Ui8yHZCOa6Savipws\nJ/r66wUGQginzU53Zk6RYxGRk+NumHodZJGTLdm9Ua9BmQ+L+fWnCXkiIiIiIpEGxyIiIiIikQbH\nIiIiIiKRBsciIiIiIpEGxyIiIiIikZZyExERERGJFDkWEREREYk0OBYRERERiTQ4FhERERGJNDgW\nEREREYk0OBYRERERiTQ4FhERERGJNDgWEREREYk0OBYRERERiTQ4FhGZATPbaGafNbNdZjZuZjvM\n7DozWzkf7cjSMxuvnXhNmOLriZPZf1nczOxVZvYxM/u+mQ3E18wXjrOtBf0+qB3yRESOwszOAH4E\n9ABfA+4DLgAuA+4HnhdCODBX7cjSM4uvwR3ACuC6BsVDIYQ/na0+S3Mxs9uB84Eh4HHgHOCLIYTX\nHWM7C/59sDCfNxcRWSQ+gb+Rvz2E8LHkpJl9FHgX8MfAm+ewHVl6ZvO10xdCuGbWeyjN7l34oPgh\n4BLgu8fZzoJ/H1TkWERkGjHK8RCwAzgjhFDLlHUCuwEDekIIwye7HVl6ZvO1EyPHhBB6T1J3ZQkw\ns0vxwfExRY4Xy/ugco5FRKZ3WTzemH0jBwghDAI/BNqBC+eoHVl6Zvu102JmrzOz95nZO8zsMjPL\nz2J/RaayKN4HNTgWEZne2fH4wBTlD8bjWXPUjiw9s/3aWQfcgP/5+jrgP4AHzeyS4+6hyMwsivdB\nDY5FRKbXFY/9U5Qn51fMUTuy9Mzma+dzwAvxAXIH8FTgr4Be4Ftmdv7xd1PkqBbF+6Am5ImIiCwR\nIYRrJ526G3izmQ0B7wGuAV4x1/0SWUgUORYRmV4Syeiaojw53zdH7cjSMxevnU/F4wtOoA2Ro1kU\n74MaHIuITO/+eJwqB+7MeJwqh26225GlZy5eO/viseME2hA5mkXxPqjBsYjI9JK1PF9kZoe9Z8al\nh54HjAC3zFE7svTMxWsnWR3g4RNoQ+RoFsX7oAbHIiLTCCFsA27EJyy9bVLxtXik7YbnODjiAAAg\nAElEQVRkTU4zK5rZOXE9z+NuRyQxW69BM9tsZkdEhs2sF/jL+ONxbQcskrXY3we1CYiIyFE02O50\nK/AcfM3OB4CLku1O40BjO/DI5I0WjqUdkazZeA2a2TX4pLvvAY8Ag8AZwBVAK/BN4BUhhIk5eCRZ\nZMzs5cDL44/rgMvxvzR8P57bH0L4vVi3l0X8PqjBsYjIDJjZk4A/BF4MrMJ3cvoqcG0I4VCmXi9T\n/EfhWNoRmexEX4NxHeM3A88gXcqtD7gdX/f4hqBBgUwhfrj64DRV6q+3xf4+qMGxiIiIiEiknGMR\nERERkUiDYxERERGRSINjEREREZFI20cvUGZ2Jb7Uyb+EEG6f396IiIiILA0aHC9cVwKXADvwmcQi\nIiIicpIprUJEREREJNLgWEREREQk0uD4OMQtOD9lZg+Y2YiZ9ZnZXWb2F2a2JVOvxcxebWafN7M7\nzGy/mY2Z2SNm9sVs3cw1V5pZwFMqAD5nZiHztWOOHlNERERkydEmIMfIzH4X+DMgH08NA2VgRfz5\n5hDCpbHuLwH/Gs8HfCeiNnybToAK8MYQwg2Z9l8D/DnQDRSBAWA004XHQgjPnt2nEhERERFQ5PiY\nmNmrgb/AB8ZfBs4NISwLIazEtz98HXBr5pKhWP8FwLIQQncIoQ04FbgOnxD512a2KbkghPCPIYR1\n+L7jAO8IIazLfGlgLCIiInKSKHI8Q2ZWxPcJ3wD8fQjh12ahzb8B3ghcE0K4dlLZTXhqxVUhhOtP\n9F4iIiIicnSKHM/cC/GBcRX4f2apzSTl4nmz1J6IiIiInACtczxzF8bjHSGEnTO9yMy6gbcBLwHO\nBrpI85UTp8xKD0VERETkhGhwPHNr4/HRmV5gZucC/5G5FmAQn2AXgBKwEuiYpT6KiIiIyAlQWsXJ\n9Tl8YHwb8GKgM4SwPISwNk66e3WsZ/PVQRERERFJKXI8c3vi8dSZVI4rUFyA5yi/bIpUjLUNzomI\niIjIPFHkeOZuicenmdmGGdTfGI/7pslR/vlprq/Fo6LKIiIiInNEg+OZ+w6wE59M9yczqN8fj2vN\nrGdyoZk9FZhuObiBeFwxTR0RERERmUUaHM9QCKEMvCf++Foz+5KZnZOUm1m3mf2Wmf1FPLUVeByP\n/P6jmT051iua2SuBf8M3CZnKPfH4SjPrms1nEREREZHGtAnIMTKzd+OR4+SDxRC+DXSj7aNfge+k\nl9QdBFrwVSoeBd4P3AA8EkLonXSfc4A7Yt0KsBffpvrxEMLFJ+HRRERERJY8RY6PUQjho8Az8JUo\ndgBFfFm2O4E/B96VqftV4OfwKPFgrPsI8Kexjcenuc99wC8A/wdP0ViHTwbcONU1IiIiInJiFDkW\nEREREYkUORYRERERiTQ4FhERERGJNDgWEREREYk0OBYRERERiTQ4FhERERGJNDgWEREREYk0OBYR\nERERiTQ4FhERERGJNDgWEREREYk0OBYRERERiQrz3QERkWZkZtuB5cCOee6KiMhi1AsMhBBOm+sb\nN+3g+JfXnxoAClj9XC7n3+fMj/lM4NziuVysbyFM03qmzfh9PhfbylxXi98nx8CRZZhlztX8GOtV\natV6WSXe52AoA7BjuK9eNlypAHB6xwoAludb6mWjsc1dowMA7K+N18tK8fkPjoymnRCR2bK8ra2t\ne/Pmzd3z3RERkcVm69atjI6Ozsu9m3ZwnAyK85nBZ958MJgMZHMhLbNweH3LJJzUB7XxkMsUWjKY\nnlyXdBCelFYzA+ekXnYIXgmHD94LmSFrOdZcG4oAdHSk/70dqtUOq1+OA2iAYvC+bmrvAmDFRDo4\nfnxsAJHFxsx2AIQQeue3J0e1Y/Pmzd233nrrfPdDRGTR2bJlC7fddtuO+bi3co5FRERERKKmjRyL\niMy3u3f203v1N+a7GyJyHHZ85Ir57oLMk6YdHCcpFLlGaRUxzaGQSwPnuRhErydCZK5L8oiTLIxs\nrnLSfkjyizOXWe3wc7lMrnLSVs3SxAqb1FYl2/eYO5zLeVlHLb1PW8ylnoh1xjP3qcZvQ+zLikKx\nXtba0YWIiIiIpJRWISILjrnfMbN7zGzMzHaa2V+aWcNPdGbWYmZXm9ldZjZiZgNm9n0z+2/TtP8O\nM7t3cvtmtiPJaxYRkaWnaSPHhVweODxynDxsEvnNRnInR4xLlk/LYrQ2ua5CuopENZkMF8tqmevG\nu33ViOW96wEYvv+xellucDy2nbmPxXBwMvEvpOHh+uoWMdqdz0wKLIfYn1g9Owkx6WklXj6RWQGj\nLd+0v35Z/K4D3g7sBv4aKAO/DDwHKAETSUUzKwHfBi4B7gM+DrQDrwL+0cyeHkJ436T2Pw68BdgV\n258AXgZcABTj/WbEzKaacXfOTNsQEZGFQ6MjEVlQzOwifGC8DbgghHAwnn8/8F1gPfBI5pL34APj\nbwEvCyFUYv1rgZ8A7zWz/z+E8KN4/vn4wPgB4DkhhL54/n3AvwOnTGpfRESWkKYdHCdx1fxh5w5f\nkzgbOU4kEdlCZo218RiJfaA8BMB+S5dD62hvA2BdazsAy6vpHQvnnw5ApeiNjbG+Xtb9aL+fO9Cf\n3jv2uhrzkC3bv4L/qqq1pCzTwUlLJlczZcnycfmkqUy0uFzLJC6LLBxXxeMfJwNjgBDCmJm9Fx8g\nZ70R/3/Bu5OBcay/18w+BHwG+E3gR7HoDZn2+zL1J2L7PziWzoYQtjQ6HyPKzzyWtkREZP4p51hE\nFppkQHlzg7IfkGYLYWadwJOBXSGE+xrU/494fEbmXPJ9o0HwLUClwXkREVkiNDgWkYUmmXS3Z3JB\njAzvb1B39xRtJedXzLD9KnBgxj0VEZGm07RpFcnctmTLaIB8LtkaOpZlshZyyUS3WDaU2WXupv4n\nANhV9m0Mq9U0HaF10JdGe7SzE4DuznQy/YY9/t/lp5/VC8DEisz20aMe/Cr2DR3R91xMdwiZDibp\nFLmYH1FtkBKRj+vDVTIT+apxll4lHLkj32HL1YksHEmu0Vrg4WyBmRWA1cDjk+qum6Kt9ZPqASRb\nQzZqPw+sAnYec69FRKQpNO3gWEQWrdvw1IpLmDR4BS4mM5UghDBoZtuA083szBDCg5PqX5ZpM/Ez\nPLXi4gbtX8gsvi+et6GLW7WRgIjIotK0g+P6pLtMdDQ3aWOQwyLHMaaabAxy72gaaHpk2ANNpZiF\nUsinF06M+YpSe8b9L7HD42nEOdmAY3lbKwCnru+plw30dABQfKI9rT8w7H2P/cvGhnP5OFmvHjFO\n+1CL3yePWsuWxWiyxSXc8pmyXE5ZNbIgXY9PoHu/mX0ts1pFK/DhBvU/+3/bu/foOqsyj+Pf55yT\npEnTpPcW0ktsKVCFQYFBxAtULl7AEcQLsnAJs0atzvKCg6OjsgZUHEddIzMqgsOMDoxrHBXwMuKI\nC6zcRoRy0UILhbZAm95S2rS5NuecPX/s/b7v7mkSaJvmcvL7rJX1Ju9+zz77TU9Pdp48z97ANcDX\nzOzCkBqBmc0EroyuSdyEL+JL+u8I19cCXz4M9yMiIuNI1U6ORWR8cs7dZ2bfBD4KrDKzn5Ctc7yT\n/fOLvw68JbQ/Zma349c5fhcwG/iqc+7eqP/fmdl3gQ8Cj5vZLaH/t+HTL9rY93dTERGZQBQ6FJGx\n6OP4yXEH8CHgvfiNPs4i2gAE/BJswNnA58Kpj+KXa1sLXOyc+/QA/X8Y+CTQCSwHLsavcXw20ESW\nlywiIhNM1UaOCyF7oBClDiQpBcn6xrl9CtJCoVsoXNvc35u21BZ80V05pF7sk44QKv/yBX9u7qw5\nadMxi5YA0LbV1w5NnlSXtnV1+/4n1WdjmLynYpxR9Vy2XnFun+eN79GlRXfRA8OOffWzfKFgV8ee\ntKm7O/tcZCxx/sX8rfBRqXWA63vxKREvKS3COVcGvhE+Uma2BGgEVh/YiEVEpFoociwiE46ZzTWz\nXMW5Bvy21QC3jfyoRERkLKjayHG601308y85Z2GpszhunBTwFUPB25yoeO7EBX6VqAce/hMAnT1Z\nVLk/pCY21PnCusn19VmftT5qO2P69PAc2e5506b7pd/mzMqep/vetfv0ubOQjbCx1+9LkCuH/QmG\niHrX5GrSljOWXwDAyWefBcDu9mzpuJ/d8H1EJqhPAO81sxX4HOa5wJnAPPw21D8evaGJiMhoqtrJ\nsYjIEH4DnACcA0zH74r3FPAvwLUuyVESEZEJp2onx2lubvQzLhd2/8jnk6XPsqhy8o3oDqfmz8v2\nFGhs8LnCi+a3ALBtxwtpW89eXxs0pdHn9DbWZn3u2OE3D+nq85Hg9ijfd/duv/Tb8YsWpOfqp/qI\n77Z8uD7KCT6pJ9xOGGlNlJCcfJo8c7luUtrWNK0RgA1/+LUf5xEtadtffOTdiExEzrk7gTtHexwi\nIjL2KOdYRERERCTQ5FhEREREJKjatIpC3he/5aPMwWRju2RnPItK8pJd6XrDRQ31WWpC+26f09DV\n0w1Ay5yZadvUqdMA2PaC31GvlmLaVl/j0zGeenYTAEtal6RtDc1+fGs2Zikak/J+d72tW9sBeNPR\nr0zbZjT5ce1c/3y4wbggL9xDUphXnxXkNbS8DIDiDj+GjU89mbaV6rYBcNpb3rNfXyIiIiITkSLH\nIiIiIiJB1UaOkyhqIZdFWEM9XhovzkW/G1go3Ouv8+c6e7JNuJLivqWLFwKwpyNbDq15ymQASkV/\n/fbN2c62FpZuW9RyJADdPV1p29bNPpLb1ZctC9e710emjwobdpz30UvTtqUvPwWAH1z2MQB2rH9u\nv+cphyXgpkxuTNtmzprnjyf6x3ds2Z62/eMXvgjA+65ARERERFDkWEREREQkVb2RY9t/i+gkD9lC\nCNmifOTkur1WAqB7b1/aNqvZR3Jrw3drxpwsH3lKvY8cd+7wecJTG2vTtuYpPoLb4HwOcNuOXWlb\n797ecOxJz/V0+efeUe/bul6II81bAdhT9OPKxxHx9F797zr5SdkYekJkutzv85k3tm3J+nwuiz6L\niIiIiCLHIiIiIiIpTY5FRERERIKqTavIVRwBcL5gLUm5yEcpFy58Wts0BYCyK6Vtff1+ebbmxqkA\n1Nfl07bOLp/60NTc4PskK7Ar9/od7qY0zwZg/oymrM89PjWjvaOcnmsJy8L19vs+bvi3G9O2/G5f\n8Ld0px/LpHx2Z+Uw+GTDv76uLB3jpz+9DYAPL18OwOZt29K2U858IyIiIiKSUeRYRAQwsxVmcSWC\niIhMRFUbOU5jwi6LzLpkKTfzkd+6QhYBLpZ9Y1/RF6519/anbV27dwPQvt0vg/byJYuyPsv+ut4e\nX1g3efKUbAxl/9xHHeGjyuuf25225cLzXfHBD6Tn2rc8BcCDq54BYH5YJg7gZR3+n6q/rjeMN7uv\nnEsi4b7Prqjwb+OzftOQDc8+C0BzY312X7/7AyIiIiKSqdrJsYjIaFu1qYPWz/xytIcx5m34yrmj\nPQQRkZTSKkRk3DGzU8zsv81sk5n1mdlmM7vDzN4dXXOpmd1iZuvMrMfMdpvZfWZ2SUVfrSGd4vTw\ntYs+VozsnYmIyGir2shxPh92yIvm/8ln6bksM4Ga8PneUHy3ZXu2k1x9g0+/eMMJSwHY1bEzbWuu\nqwNgVkiB2BSlNLTO8gV227f5NZCbc9mue8tecbTv8zWvSs898pAv4Htuywt+vMUstaMc0jzqwm54\n5KKCvJCikaSQNOayf9Y5W/14ih0+paNj5Z/Strpd3YiMN2b2AeA7QAn4ObAWmA2cDHwE+FG49DvA\n48DdwGZgBvBW4GYzO8Y5d2W4bhdwNXApsDB8nthwGG9FRETGoKqdHItI9TGzlwPXAbuB1zvnHq9o\nnxd9eZxz7pmK9lrgV8BnzOx659wm59wu4CozOwNY6Jy76gDHtHKQpmMPpB8RERkbqnZybKH6Lh9F\nWPNhrbPaEH1NI66AhUjz5LJfwi1fk7XV5/yya7Nn+EjwKSdnBXk7XvDR3lWPPAxAU9P0tG3OvBYA\n1j3tfz43REV05V4ffV63JvvZPn3qHACWLvXPvXNjtptdcZqPANeEXfOaLLuvfvP9lkOBYUMuKzSc\n1ukjzmu/fB0Anf3Zzn91VfuvL1Xsw/j3rS9WTowBnHMbo8+fGaB9r5l9G3gjcCZw02Ecq4iIjEOa\nHonIeHJqOP7qxS40swXAp/GT4AVAfcUlLcMxIOfcSYM8/0rgxOF4DhERGTlVOzlOcnPzlkVRk+Xd\nXIgq1+Wz2y+Exrklf/30pmzDjhOX+L/U9nb7qPLSV56atk1u8huD5MLzdPdkm4A0zzoCgFfOXgLA\nIw/cmbZ1vuA34/jNb7NzS49Z7MfX6/OeG6bVpW3r5ofodetcf+KZtmwMfX5cuRD1LpeyDUwmh41O\n+sKSc0a88YnqMWXcmRqOm4a6yMwWAX8ApgH3AHcAHfg85Vbg/UDdYI8XEZGJq2onxyJSlZKK1xZg\nzRDXfRJfgHeZc+77cYOZvRc/ORYREdmPQociMp78Phzf8iLXHRWOtwzQdvogjykBmEV/bhIRkQmn\naiPHNeHnm1mURhDSKZJzFhXr1YbPJ032xyku+4try8yZACyZ5wvmmmuz55k0xRfgtRwxy39NVvDW\n09cJQKfzu+bVFhqy8dU0hmuyIr3HHlsLQHefX2JtfnhegN5+/0/V9GfzAeiYMy1t6wzpEQ8/+BAA\nuzp70rbmHX4Jt7Om+HSMfoppWy5KsRAZJ74DLAeuNLNfO+eeiBvNbF4oytsQTp0B/CJqfxPwV4P0\nvSMcFwDrh2Owx7U0s1IbXIiIjCtVOzkWkerjnHvCzD4CXA88YmY/w69zPAP4c/wSb8vwy71dBvzY\nzH4CtAHHAW/Gr4P8ngG6vxN4F3Crmd0O9ADPOuduPrx3JSIiY0nVT47zUdFZIUSHk2ByXRxVDlFU\nm+aju+84+fi0rb7gQ8WbO3wkuLwqTnVcDUDPNh/1bZg1I22ZFDbj2N72JAC1tdlY1rb7iG4+V5Oe\nO3bRAgBOWjgbgDUPPJm29fX4pd82TPLn9kQjmDvbR4V3dfqIs4s2N1mU8/dTE+6vFN1zjmy5OpHx\nwjn3r2a2CrgCHxk+H2gH/gjcGK75o5ktA74EnIt/r3sMeAc+b3mgyfGN+E1ALgL+Njzmd4AmxyIi\nE0jVT45FpPo45/4PuPBFrrkfv57xQPbLKXLOlYDPhg8REZmgqnZybLZvlBiy6sNc+Cxe1qw2RHn3\nhFMdXV1pW1+vj9Pu6PP5ui8Us8jxopk+Utxd9hHgB5/IUhXLRR/CParF5yrPnJ493+tOfT0Al37q\n8vTcD799o+/j7vsBmL69I207p8HnH29+1O9xsLE/yyvuaParWi0J6c4zoxWqFjT4yHEp5FvHseI4\n51pEREREtFqFiIiIiEhKk2MRERERkaBq0yqyFIqoAC3kWNSE3fPigrSa6X7jrde8/SwA/vTog2nb\nlId8KsPCvE+d2Ls4W2KtVPA74j3XthWAuqy+jq4+vyvdbx9eBcArarPHnVb2Y+i8ZUV6btHTWwCY\n2+c7qZ06N20r4lM0Fhb8EnBH1mXLwvWFHfXyOV84WHTZffWY3y3Pyvt+DwBcKarcExERERFFjkVE\nREREElUbOS6ETUDyUaQ0HyLG+WTZtvj6EBU+57y3A3D2BW9L2+760OcBKK33hW+5Ne1pW++abQDM\nOLoZgPW72tK2Y1r9kmzFsOFt7qmtaVtPiDg/8fSP0nPlgh9RXV29H185i+zWON+2t7TX91kqxYP3\n47MQMY6q7upKYQm3UHxXctnjku+HiIiIiHiKHIuIiIiIBJoci4iIiIgEVZtWkWRT5KMUg7Q4LxcK\n86Ld88rbfarE6u/5NIfGI7PiOdpCGkXeP86RpTvUhbWMG5/3ayEffdpxadsxc0J1Xnjams5sbWI6\n/ePytVkFXyGkOfT3+wK7qLYPqymE+/Kd5YvFtC0pwOsPW+Plc1nCSHKVCykaheienWmHPBEREZGY\nIsciIiIiIkHVRo6TQrwCWdFZIRSlFfL+tmuj3w1KYcmzth/d7q+pmZR1lvNtSZGfiyr5imGNtJl7\n/LEr29SOwmK/e9706f5kx5Qo2tsZtrOLll1zRb/0m4UIcH8UAc4lweqy2+deAFypuE9XcWQ7iRin\nxyhYbE6RYxEREZGYIsciIiIiIkEVR479vD9erszCuSTVthRFZmsKdeFciO7ms+hrbd63FZ1vs2jz\njJqamtCn77T58WyZt/YFTQDsbO8CoLu3N22bXfLXuzg6nA9jDefKUYTahYhxcioXL8MW7qccxleM\ncoltnwXrwKLnM6ffjURERERimh2JiIiIiASaHIvImGJmHzOzJ8ysx8ycmX1itMckIiITR9WmVVgo\nWIvTCHLJMmjh63y8lVxIWyA8LhftQFcKv0Mkv0mUooflw+50uZBW4aLUiZqdfje7wlS/U96Wri1p\n25Jco78+TnMI47NQ8ZeLUiLKOd9/PqReFF2W2lEOu95ZWikY3XNyP+Fes0dlRXoiY4WZXQT8M/AI\ncC3QB/x+VAclIiITStVOjkVkXDovOTrn2oa8chxYtamD1s/88oAes+Er5x6m0YiIyEtRvZPjUDTn\nok0vLJ9EZv3XyXJvAMWKKGo5elwSdc0lRX75+ELfVgqR3Pgb2hSiw694z0UAbFnzjbQtv6HTPzyO\nHCefJ/1HhX/lMIY0OhwN14XivFLYPASLo9GhszA+i6Ll+Xz1/vPLuHUkQDVMjEVEZHxSzrGIjDoz\nu8rMHLAsfO2Sj+jrFWY218xuNLNNZlYys0ujPo4ws2+b2QYz22tm283sVjM7aZDnbDaza81so5n1\nmtkaM/ukmS0Kz/f9Ebh1EREZY6o2dJhEYXNRFLVcDHnEIU/Yuf2XPKux/dtcEsENjysUCvu3JZHm\ncva4hl7fZ8u8owA45fiT07bixnt8X/GOIi6JQrvKrtKNS9IdsKPxJYHmQsEvK9fXvzd7YHhAsjV1\n9O2g7JRzLGPGinC8FFgIXD3ANdPx+cedwK34v59sBTCzlwH34iPPdwH/BcwH3gWca2YXOuf+J+nI\nzCaF607E5zf/AGgGPge8fljvTERExpWqnRyLyPjhnFsBrDCzM4CFzrmrBrjseOBm4C+dc8WKtuvx\nE+PPO+euSU6a2XXA3cB/mNlC51xnaPoUfmL8Q+BiF34bNrNrgIcPZOxmtnKQpmMPpB8RERkblFYh\nIuPFXuCKyomxmc0DzgGeA74atznn7sdHkacD74ia3o+PPP+di/5M5Jx7Hr9KhoiITFBVGzlOlnKL\nJcuapbvGxYVr4cdj8lMyfnyy217yM7RUzJZ5qwkpFklbLp89rtDgd9Zr373L97l9VzaWUlIgFxX+\nVaSCuJqs8i/pP8nCiDbBSwv3wmpvaQqFf1xyQ/4Qp1LEKRYi48AG59y2Ac6/Khzvcc71D9B+F3BJ\nuO4mM2sCFgPPO+c2DHD9vQcyKOfcYDnNK/HRaRERGUcUORaR8WLLIOebw3HzIO3J+anh2BSOWwe5\nfrDzIiIyAVRt5DgroosiuWENtmyVtihqG3b2cKEKLln2DbKIMy4O13rFon+efI3/VhaiGre9z28H\nYN3P7wSg/NjatC2J2u4ToU4K/cLzFMtZhDq5rj9EfkvRJiW5yqh3PMzQV/J8+xYoqiBPxpX9/wN6\nHeE4d5D2Iyqu2x2Ocwa5frDzIiIyAVTt5FhEJoxHwvF1ZlYYoFhvWTg+DOCc221m64BWM2sdILXi\ndcM1sONamlmpTT1ERMYVpVWIyLjmnNsI/AZoBT4Rt5nZq4GLgZ3AbVHTTfj3v38wy/6cYmbzK/sQ\nEZGJpWojx0kaQrwLXPIzMNnprhwtJJxPr99/PeBS2D3PBqhgK4THFcL6yFabFcP1bwq1Q7e2A1AT\n/SrSH1Io4rWMy6F7F56vFK+1nOzEV9p/F7ykyC65n3gXvHJaiBfSRqKxD5AlIjJeLQfuA75mZucA\nD5Gtc1wGLnPO7Ymu/ypwPnARcIyZ3YHPXX43fum389lnH0oREZkoqnZyLCITh3NunZmdDHweeCtw\nBj63+H+Ba5xzD1Zc32Nmy4AvAO8ELgfWA18G7sFPjndzaFpXr17NSScNuJiFiIgMYfXq1eD/Ijji\nzCl8KCKSMrMPAN8FljvnbjiEfvrwG1g+NlxjExlmyUY1a0Z1FCIDOwEoOefqRvqJFTkWkQnJzI50\nzrVVnFsAXAkUgV8c4lOsgsHXQRYZbcnujnqNylg0xO6jh50mxyIyUd1iZjXASmAX/s935wEN+J3z\n2oZ4rIiIVClNjkVkoroZeB9wIb4YrxN4APiWc+7W0RyYiIiMHk2ORWRCcs5dB1w32uMQEZGxResc\ni4iIiIgEmhyLiIiIiARayk1EREREJFDkWEREREQk0ORYRERERCTQ5FhEREREJNDkWEREREQk0ORY\nRERERCTQ5FhEREREJNDkWEREREQk0ORYROQlMLN5ZvbvZtZmZn1mtsHMrjWzaaPRj0il4Xhthce4\nQT62HM7xS3Uzs3ea2TfN7B4z2x1eU/95kH0d1vdRbQIiIvIizGwxcD8wG/gZsAY4BVgGPAm81jm3\nY6T6Eak0jK/RDcBU4NoBmjudc18frjHLxGJmjwInAJ3ARuBY4AfOuUsOsJ/D/j5aOJQHi4hMENfh\n34g/5pz7ZnLSzP4JuBy4Blg+gv2IVBrO19Yu59xVwz5Cmegux0+KnwZOB357kP0c9vdRRY5FRIYQ\nohRPAxuAxc65ctQ2BdgMGDDbOdd1uPsRqTScr60QOcY513qYhiuCmZ2BnxwfUOR4pN5HlXMsIjK0\nZeF4R/xGDOCc2wPcBzQAp45QPyKVhvu1VWdml5jZZ83s42a2zMzywzhekYM1Iu+jmhyLiAztmHB8\napD2teF49Aj1I1JpuF9bc4Gb8X+evha4C1hrZqcf9AhFhseIvI9qciwiMrTmcN/qlTMAAAI/SURB\nVOwYpD05P3WE+hGpNJyvre8BZ+InyJOB44EbgFbgV2Z2wsEPU+SQjcj7qAryREREBADn3NUVp1YB\ny82sE/gb4CrggpEel8hIUuRYRGRoSSSieZD25PyuEepHpNJIvLauD8c3HEIfIodqRN5HNTkWERna\nk+E4WA7bknAcLAduuPsRqTQSr63t4Tj5EPoQOVQj8j6qybGIyNCStTjPMbN93jPD0kGvBbqB349Q\nPyKVRuK1lVT/rzuEPkQO1Yi8j2pyLCIyBOfcM8Ad+IKkv65ovhofSbs5WVPTzGrM7NiwHudB9yPy\nUg3Xa9TMlprZfpFhM2sFvhW+PKjtfkUOxGi/j2oTEBGRFzHAdqWrgVfj19x8Cjgt2a40TCTWA89W\nbqRwIP2IHIjheI2a2VX4oru7gWeBPcBi4FxgEnA7cIFzbu8I3JJUGTM7Hzg/fDkXeBP+LxH3hHPt\nzrkrwrWtjOL7qCbHIiIvgZnNB74AvBmYgd+J6Tbgaufczui6VgZ5Uz+QfkQO1KG+RsM6xsuBV5Et\n5bYLeBS/7vHNTpMGOUjhl6+/H+KS9PU42u+jmhyLiIiIiATKORYRERERCTQ5FhEREREJNDkWERER\nEQk0ORYRERERCTQ5FhEREREJNDkWEREREQk0ORYRERERCTQ5FhEREREJNDkWEREREQk0ORYRERER\nCTQ5FhEREREJNDkWEREREQk0ORYRERERCTQ5FhEREREJNDkWEREREQk0ORYRERERCTQ5FhEREREJ\n/h8zJLVNVxwg3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ffec15940>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
